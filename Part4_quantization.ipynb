{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a012a9a8",
   "metadata": {},
   "source": [
    "# Part 4: Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93caa31d",
   "metadata": {},
   "source": [
    "## Load  CIFAR10 dataset from tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3aeb496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 03:07:38.270551: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/xilinx/xrt/lib:/usr/lib:/usr/lib/x86_64-linux-gnu:/usr/local/lib:/opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib\n",
      "2021-11-30 03:07:38.270582: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "#x_train = np.expand_dims(x_train, -1)\n",
    "#x_test = np.expand_dims(x_test, -1)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a9d3d",
   "metadata": {},
   "source": [
    "### Let's print some information about the dataset\n",
    "Print the the dataset shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74762a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (10000, 32, 32, 3) (50000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape,y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e1ebbd",
   "metadata": {},
   "source": [
    "## Construct a model\n",
    "This time we're going to use QKeras layers.\n",
    "QKeras is \"Quantized Keras\" for deep heterogeneous quantization of ML models.\n",
    "\n",
    "https://github.com/google/qkeras\n",
    "\n",
    "It is maintained by Google and recently support for QKeras model is added to hls4ml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80e1ffd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 03:07:42.907327: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/xilinx/xrt/lib:/usr/lib:/usr/lib/x86_64-linux-gnu:/usr/local/lib:/opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib\n",
      "2021-11-30 03:07:42.907354: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-11-30 03:07:42.907388: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ECE-util1): /proc/driver/nvidia/version does not exist\n",
      "2021-11-30 03:07:42.907664: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-30 03:07:42.937231: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200000000 Hz\n",
      "2021-11-30 03:07:42.939483: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e93b9c7420 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-11-30 03:07:42.939508: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "q_conv2d (QConv2D)           (None, 30, 30, 16)        448       \n",
      "_________________________________________________________________\n",
      "q_activation (QActivation)   (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "q_conv2d_1 (QConv2D)         (None, 28, 28, 16)        2320      \n",
      "_________________________________________________________________\n",
      "q_activation_1 (QActivation) (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "q_conv2d_2 (QConv2D)         (None, 12, 12, 8)         1160      \n",
      "_________________________________________________________________\n",
      "q_activation_2 (QActivation) (None, 12, 12, 8)         0         \n",
      "_________________________________________________________________\n",
      "q_conv2d_3 (QConv2D)         (None, 10, 10, 8)         584       \n",
      "_________________________________________________________________\n",
      "q_activation_3 (QActivation) (None, 10, 10, 8)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "q_dense (QDense)             (None, 10)                2010      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 6,522\n",
      "Trainable params: 6,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu\n",
    "from qkeras.qconvolutional import QConv2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout,Flatten,InputLayer, MaxPooling2D, Activation\n",
    "\n",
    "input_shape=( 32, 32, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=input_shape))\n",
    "model.add(QConv2D(16, kernel_size=(3, 3),kernel_quantizer=quantized_bits(6,0,alpha=1),  bias_quantizer=quantized_bits(6,0,alpha=1)))\n",
    "model.add(QActivation(activation=quantized_relu(6)))\n",
    "model.add(QConv2D(16, kernel_size=(3, 3),kernel_quantizer=quantized_bits(6,0,alpha=1),  bias_quantizer=quantized_bits(6,0,alpha=1)))\n",
    "model.add(QActivation(activation=quantized_relu(6)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(QConv2D(8, kernel_size=(3, 3),kernel_quantizer=quantized_bits(6,0,alpha=1),  bias_quantizer=quantized_bits(6,0,alpha=1)))\n",
    "model.add(QActivation(activation=quantized_relu(6)))\n",
    "model.add(QConv2D(8, kernel_size=(3, 3),kernel_quantizer=quantized_bits(6,0,alpha=1),  bias_quantizer=quantized_bits(6,0,alpha=1)))\n",
    "model.add(QActivation(activation=quantized_relu(6)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(QDense(10,kernel_quantizer=quantized_bits(6,0,alpha=1),  bias_quantizer=quantized_bits(6,0,alpha=1)))\n",
    "model.add(Activation(activation='softmax'))\n",
    "\n",
    "model.build()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962dcbb8",
   "metadata": {},
   "source": [
    "## Train sparse\n",
    "Let's train with model sparsity again, since QKeras layers are prunable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcb15285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vitis-ai-user/.conda/envs/cifar10/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:218: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "pruning_params = {\"pruning_schedule\" : pruning_schedule.ConstantSparsity(0.75, begin_step=0, frequency=100)}\n",
    "model = prune.prune_low_magnitude(model, **pruning_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a68b397",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "We'll use the same settings as the model for part 1: Adam optimizer with categorical crossentropy loss.\n",
    "The callbacks will decay the learning rate and save the model into a directory 'model_mnist_cnn4'\n",
    "The model isn't very complex, so this should just take a few minutes even on the CPU.\n",
    "If you've restarted the notebook kernel after training once, set `train = False` to load the trained model rather than training again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f66a031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2021-12-02 01:20:06.489221: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\n",
      "2021-12-02 01:20:07.226135: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 491520000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "  1/313 [..............................] - ETA: 0s - loss: 1.5980 - accuracy: 0.3516"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 01:20:09.112291: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/313 [..............................] - ETA: 1:17 - loss: 1.5890 - accuracy: 0.3789WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0462s vs `on_train_batch_end` time: 0.4550s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 01:20:09.363365: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: model_cifar10_cnn/logs/train/plugins/profile/2021_12_02_01_20_09\n",
      "2021-12-02 01:20:09.386325: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to model_cifar10_cnn/logs/train/plugins/profile/2021_12_02_01_20_09/ECE-util1.trace.json.gz\n",
      "2021-12-02 01:20:09.438190: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: model_cifar10_cnn/logs/train/plugins/profile/2021_12_02_01_20_09\n",
      "2021-12-02 01:20:09.456292: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to model_cifar10_cnn/logs/train/plugins/profile/2021_12_02_01_20_09/ECE-util1.memory_profile.json.gz\n",
      "2021-12-02 01:20:09.608668: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: model_cifar10_cnn/logs/train/plugins/profile/2021_12_02_01_20_09Dumped tool data for xplane.pb to model_cifar10_cnn/logs/train/plugins/profile/2021_12_02_01_20_09/ECE-util1.xplane.pb\n",
      "Dumped tool data for overview_page.pb to model_cifar10_cnn/logs/train/plugins/profile/2021_12_02_01_20_09/ECE-util1.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to model_cifar10_cnn/logs/train/plugins/profile/2021_12_02_01_20_09/ECE-util1.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to model_cifar10_cnn/logs/train/plugins/profile/2021_12_02_01_20_09/ECE-util1.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to model_cifar10_cnn/logs/train/plugins/profile/2021_12_02_01_20_09/ECE-util1.kernel_stats.pb\n",
      "\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0462s vs `on_train_batch_end` time: 0.4550s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311/313 [============================>.] - ETA: 0s - loss: 1.6224 - accuracy: 0.3949\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.50582, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.50582, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00001: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00001: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 12s 39ms/step - loss: 1.6222 - accuracy: 0.3948 - val_loss: 1.5058 - val_accuracy: 0.4535\n",
      "Epoch 2/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.6019 - accuracy: 0.4064\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.50582 to 1.48430, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.50582 to 1.48430, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00002: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00002: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.6019 - accuracy: 0.4064 - val_loss: 1.4843 - val_accuracy: 0.4663\n",
      "Epoch 3/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.5845 - accuracy: 0.4103\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.48430\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.48430\n",
      "\n",
      "Epoch 00003: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00003: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.5849 - accuracy: 0.4104 - val_loss: 1.4881 - val_accuracy: 0.4630\n",
      "Epoch 4/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.5689 - accuracy: 0.4185\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.48430 to 1.47410, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.48430 to 1.47410, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00004: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00004: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.5688 - accuracy: 0.4186 - val_loss: 1.4741 - val_accuracy: 0.4651\n",
      "Epoch 5/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.5603 - accuracy: 0.4196\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.47410 to 1.45957, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.47410 to 1.45957, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00005: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00005: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.5602 - accuracy: 0.4196 - val_loss: 1.4596 - val_accuracy: 0.4684\n",
      "Epoch 6/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.5519 - accuracy: 0.4275\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.45957\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.45957\n",
      "\n",
      "Epoch 00006: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00006: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 12s 39ms/step - loss: 1.5520 - accuracy: 0.4275 - val_loss: 1.4676 - val_accuracy: 0.4673\n",
      "Epoch 7/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.5491 - accuracy: 0.4290\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.45957 to 1.44791, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.45957 to 1.44791, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00007: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00007: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 12s 39ms/step - loss: 1.5491 - accuracy: 0.4290 - val_loss: 1.4479 - val_accuracy: 0.4676\n",
      "Epoch 8/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.5379 - accuracy: 0.4350\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.44791 to 1.44017, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.44791 to 1.44017, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00008: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00008: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 36ms/step - loss: 1.5376 - accuracy: 0.4352 - val_loss: 1.4402 - val_accuracy: 0.4720\n",
      "Epoch 9/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.5311 - accuracy: 0.4365\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.44017 to 1.43070, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.44017 to 1.43070, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00009: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00009: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 12s 38ms/step - loss: 1.5311 - accuracy: 0.4365 - val_loss: 1.4307 - val_accuracy: 0.4794\n",
      "Epoch 10/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.5222 - accuracy: 0.4401\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.43070 to 1.42692, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.43070 to 1.42692, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00010: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00010: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00010: saving model to model_cifar10_cnn/KERAS_check_model_epoch10.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 36ms/step - loss: 1.5224 - accuracy: 0.4401 - val_loss: 1.4269 - val_accuracy: 0.4808\n",
      "Epoch 11/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.5185 - accuracy: 0.4386\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.42692\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.42692\n",
      "\n",
      "Epoch 00011: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00011: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.5185 - accuracy: 0.4386 - val_loss: 1.4296 - val_accuracy: 0.4814\n",
      "Epoch 12/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.5159 - accuracy: 0.4446\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.42692 to 1.42210, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.42692 to 1.42210, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00012: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00012: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 36ms/step - loss: 1.5159 - accuracy: 0.4446 - val_loss: 1.4221 - val_accuracy: 0.4868\n",
      "Epoch 13/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - ETA: 0s - loss: 1.5057 - accuracy: 0.4473\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.42210 to 1.41123, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.42210 to 1.41123, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00013: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00013: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 36ms/step - loss: 1.5057 - accuracy: 0.4473 - val_loss: 1.4112 - val_accuracy: 0.4876\n",
      "Epoch 14/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.5061 - accuracy: 0.4482\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.41123\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.41123\n",
      "\n",
      "Epoch 00014: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00014: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.5061 - accuracy: 0.4482 - val_loss: 1.4224 - val_accuracy: 0.4824\n",
      "Epoch 15/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4974 - accuracy: 0.4496\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.41123\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.41123\n",
      "\n",
      "Epoch 00015: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00015: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 13s 43ms/step - loss: 1.4974 - accuracy: 0.4496 - val_loss: 1.4151 - val_accuracy: 0.4846\n",
      "Epoch 16/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.4914 - accuracy: 0.4521\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.41123 to 1.40118, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.41123 to 1.40118, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00016: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00016: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 13s 40ms/step - loss: 1.4913 - accuracy: 0.4523 - val_loss: 1.4012 - val_accuracy: 0.4904\n",
      "Epoch 17/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4863 - accuracy: 0.4561\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.40118\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.40118\n",
      "\n",
      "Epoch 00017: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00017: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 36ms/step - loss: 1.4863 - accuracy: 0.4561 - val_loss: 1.4021 - val_accuracy: 0.4928\n",
      "Epoch 18/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4846 - accuracy: 0.4582\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.40118\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.40118\n",
      "\n",
      "Epoch 00018: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00018: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.4846 - accuracy: 0.4582 - val_loss: 1.4066 - val_accuracy: 0.4855\n",
      "Epoch 19/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4793 - accuracy: 0.4588\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.40118 to 1.39339, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.40118 to 1.39339, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00019: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00019: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 12s 37ms/step - loss: 1.4793 - accuracy: 0.4588 - val_loss: 1.3934 - val_accuracy: 0.4944\n",
      "Epoch 20/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4770 - accuracy: 0.4610\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.39339\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.39339\n",
      "\n",
      "Epoch 00020: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00020: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00020: saving model to model_cifar10_cnn/KERAS_check_model_epoch20.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 12s 39ms/step - loss: 1.4770 - accuracy: 0.4610 - val_loss: 1.4067 - val_accuracy: 0.4947\n",
      "Epoch 21/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4735 - accuracy: 0.4631\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.39339 to 1.38911, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.39339 to 1.38911, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00021: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00021: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 12s 40ms/step - loss: 1.4735 - accuracy: 0.4631 - val_loss: 1.3891 - val_accuracy: 0.4967\n",
      "Epoch 22/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4738 - accuracy: 0.4617\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.38911\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.38911\n",
      "\n",
      "Epoch 00022: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00022: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 12s 40ms/step - loss: 1.4738 - accuracy: 0.4617 - val_loss: 1.3906 - val_accuracy: 0.4965\n",
      "Epoch 23/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4654 - accuracy: 0.4661\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.38911 to 1.37603, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.38911 to 1.37603, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00023: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00023: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 12s 38ms/step - loss: 1.4654 - accuracy: 0.4661 - val_loss: 1.3760 - val_accuracy: 0.5013\n",
      "Epoch 24/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4675 - accuracy: 0.4677\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.37603\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.37603\n",
      "\n",
      "Epoch 00024: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00024: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.4675 - accuracy: 0.4677 - val_loss: 1.4111 - val_accuracy: 0.4927\n",
      "Epoch 25/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4628 - accuracy: 0.4656\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.37603\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.37603\n",
      "\n",
      "Epoch 00025: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00025: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 12s 39ms/step - loss: 1.4628 - accuracy: 0.4656 - val_loss: 1.3785 - val_accuracy: 0.5044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4612 - accuracy: 0.4676\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.37603 to 1.36661, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.37603 to 1.36661, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00026: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00026: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 37ms/step - loss: 1.4612 - accuracy: 0.4676 - val_loss: 1.3666 - val_accuracy: 0.5058\n",
      "Epoch 27/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4519 - accuracy: 0.4745\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.36661 to 1.36475, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.36661 to 1.36475, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00027: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00027: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.4519 - accuracy: 0.4745 - val_loss: 1.3648 - val_accuracy: 0.5077\n",
      "Epoch 28/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4526 - accuracy: 0.4695\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.36475 to 1.36390, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.36475 to 1.36390, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00028: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00028: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.4526 - accuracy: 0.4695 - val_loss: 1.3639 - val_accuracy: 0.5062\n",
      "Epoch 29/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4505 - accuracy: 0.4705\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.36390\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.36390\n",
      "\n",
      "Epoch 00029: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00029: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 36ms/step - loss: 1.4505 - accuracy: 0.4705 - val_loss: 1.3736 - val_accuracy: 0.5061\n",
      "Epoch 30/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4470 - accuracy: 0.4722\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.36390 to 1.35998, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.36390 to 1.35998, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00030: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00030: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00030: saving model to model_cifar10_cnn/KERAS_check_model_epoch30.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.4470 - accuracy: 0.4722 - val_loss: 1.3600 - val_accuracy: 0.5084\n",
      "Epoch 31/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.4397 - accuracy: 0.4762\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.35998\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.35998\n",
      "\n",
      "Epoch 00031: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00031: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.4396 - accuracy: 0.4763 - val_loss: 1.3735 - val_accuracy: 0.5054\n",
      "Epoch 32/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4413 - accuracy: 0.4755\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.35998\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.35998\n",
      "\n",
      "Epoch 00032: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00032: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.4413 - accuracy: 0.4755 - val_loss: 1.3914 - val_accuracy: 0.5082\n",
      "Epoch 33/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4355 - accuracy: 0.4752\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.35998 to 1.34759, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.35998 to 1.34759, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00033: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00033: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 37ms/step - loss: 1.4355 - accuracy: 0.4752 - val_loss: 1.3476 - val_accuracy: 0.5136\n",
      "Epoch 34/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4409 - accuracy: 0.4736\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.34759\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.34759\n",
      "\n",
      "Epoch 00034: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00034: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 36ms/step - loss: 1.4409 - accuracy: 0.4736 - val_loss: 1.3519 - val_accuracy: 0.5096\n",
      "Epoch 35/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4374 - accuracy: 0.4772\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.34759\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.34759\n",
      "\n",
      "Epoch 00035: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00035: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 12s 37ms/step - loss: 1.4374 - accuracy: 0.4772 - val_loss: 1.3842 - val_accuracy: 0.5054\n",
      "Epoch 36/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4341 - accuracy: 0.4771\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00036: val_loss improved from 1.34759 to 1.34308, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00036: val_loss improved from 1.34759 to 1.34308, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00036: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00036: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 12s 39ms/step - loss: 1.4341 - accuracy: 0.4771 - val_loss: 1.3431 - val_accuracy: 0.5176\n",
      "Epoch 37/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.4327 - accuracy: 0.4789\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.34308\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.34308\n",
      "\n",
      "Epoch 00037: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00037: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.4323 - accuracy: 0.4793 - val_loss: 1.3464 - val_accuracy: 0.5170\n",
      "Epoch 38/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4281 - accuracy: 0.4794\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.34308\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.34308\n",
      "\n",
      "Epoch 00038: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00038: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 36ms/step - loss: 1.4281 - accuracy: 0.4794 - val_loss: 1.3910 - val_accuracy: 0.5006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.4262 - accuracy: 0.4814\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.34308\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.34308\n",
      "\n",
      "Epoch 00039: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00039: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 12s 38ms/step - loss: 1.4259 - accuracy: 0.4815 - val_loss: 1.3435 - val_accuracy: 0.5133\n",
      "Epoch 40/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4224 - accuracy: 0.4818\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00040: val_loss improved from 1.34308 to 1.34060, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00040: val_loss improved from 1.34308 to 1.34060, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00040: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00040: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00040: saving model to model_cifar10_cnn/KERAS_check_model_epoch40.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 12s 37ms/step - loss: 1.4224 - accuracy: 0.4818 - val_loss: 1.3406 - val_accuracy: 0.5201\n",
      "Epoch 41/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.4173 - accuracy: 0.4850\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00041: val_loss improved from 1.34060 to 1.33946, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00041: val_loss improved from 1.34060 to 1.33946, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00041: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00041: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.4171 - accuracy: 0.4849 - val_loss: 1.3395 - val_accuracy: 0.5210\n",
      "Epoch 42/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4171 - accuracy: 0.4844\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.33946\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.33946\n",
      "\n",
      "Epoch 00042: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00042: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 12s 37ms/step - loss: 1.4171 - accuracy: 0.4844 - val_loss: 1.3474 - val_accuracy: 0.5186\n",
      "Epoch 43/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4155 - accuracy: 0.4825\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.33946\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.33946\n",
      "\n",
      "Epoch 00043: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00043: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 36ms/step - loss: 1.4155 - accuracy: 0.4825 - val_loss: 1.3436 - val_accuracy: 0.5187\n",
      "Epoch 44/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4115 - accuracy: 0.4872\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00044: val_loss improved from 1.33946 to 1.33233, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00044: val_loss improved from 1.33946 to 1.33233, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00044: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00044: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 37ms/step - loss: 1.4115 - accuracy: 0.4872 - val_loss: 1.3323 - val_accuracy: 0.5238\n",
      "Epoch 45/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.4089 - accuracy: 0.4881\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00045: val_loss improved from 1.33233 to 1.32982, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00045: val_loss improved from 1.33233 to 1.32982, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00045: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00045: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 36ms/step - loss: 1.4091 - accuracy: 0.4882 - val_loss: 1.3298 - val_accuracy: 0.5270\n",
      "Epoch 46/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4097 - accuracy: 0.4903\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.32982\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.32982\n",
      "\n",
      "Epoch 00046: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00046: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 12s 37ms/step - loss: 1.4097 - accuracy: 0.4903 - val_loss: 1.3328 - val_accuracy: 0.5244\n",
      "Epoch 47/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4073 - accuracy: 0.4874\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00047: val_loss improved from 1.32982 to 1.32568, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00047: val_loss improved from 1.32982 to 1.32568, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00047: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00047: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 12s 37ms/step - loss: 1.4073 - accuracy: 0.4874 - val_loss: 1.3257 - val_accuracy: 0.5203\n",
      "Epoch 48/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4057 - accuracy: 0.4893\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.32568\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.32568\n",
      "\n",
      "Epoch 00048: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00048: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 36ms/step - loss: 1.4057 - accuracy: 0.4893 - val_loss: 1.3257 - val_accuracy: 0.5262\n",
      "Epoch 49/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4074 - accuracy: 0.4897\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00049: val_loss improved from 1.32568 to 1.32477, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00049: val_loss improved from 1.32568 to 1.32477, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00049: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00049: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 12s 39ms/step - loss: 1.4074 - accuracy: 0.4897 - val_loss: 1.3248 - val_accuracy: 0.5276\n",
      "Epoch 50/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3967 - accuracy: 0.4940\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00050: val_loss improved from 1.32477 to 1.31022, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00050: val_loss improved from 1.32477 to 1.31022, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00050: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00050: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00050: saving model to model_cifar10_cnn/KERAS_check_model_epoch50.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 12s 39ms/step - loss: 1.3962 - accuracy: 0.4943 - val_loss: 1.3102 - val_accuracy: 0.5324\n",
      "Epoch 51/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4010 - accuracy: 0.4921\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.31022\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.31022\n",
      "\n",
      "Epoch 00051: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00051: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 12s 38ms/step - loss: 1.4010 - accuracy: 0.4921 - val_loss: 1.3271 - val_accuracy: 0.5265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.4031 - accuracy: 0.4929\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.31022\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.31022\n",
      "\n",
      "Epoch 00052: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00052: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 12s 38ms/step - loss: 1.4031 - accuracy: 0.4929 - val_loss: 1.3259 - val_accuracy: 0.5257\n",
      "Epoch 53/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.4022 - accuracy: 0.4912\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.31022\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.31022\n",
      "\n",
      "Epoch 00053: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00053: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 36ms/step - loss: 1.4021 - accuracy: 0.4912 - val_loss: 1.3652 - val_accuracy: 0.5136\n",
      "Epoch 54/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3974 - accuracy: 0.4924\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.31022\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.31022\n",
      "\n",
      "Epoch 00054: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00054: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.3974 - accuracy: 0.4924 - val_loss: 1.3137 - val_accuracy: 0.5257\n",
      "Epoch 55/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3963 - accuracy: 0.4954\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.31022\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.31022\n",
      "\n",
      "Epoch 00055: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00055: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 36ms/step - loss: 1.3963 - accuracy: 0.4954 - val_loss: 1.3524 - val_accuracy: 0.5122\n",
      "Epoch 56/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3887 - accuracy: 0.4973\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00056: val_loss improved from 1.31022 to 1.30706, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00056: val_loss improved from 1.31022 to 1.30706, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00056: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00056: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 37ms/step - loss: 1.3887 - accuracy: 0.4972 - val_loss: 1.3071 - val_accuracy: 0.5348\n",
      "Epoch 57/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3905 - accuracy: 0.4967\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00057: val_loss improved from 1.30706 to 1.30275, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00057: val_loss improved from 1.30706 to 1.30275, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00057: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00057: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 36ms/step - loss: 1.3905 - accuracy: 0.4968 - val_loss: 1.3027 - val_accuracy: 0.5338\n",
      "Epoch 58/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3923 - accuracy: 0.4940\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.30275\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.30275\n",
      "\n",
      "Epoch 00058: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00058: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.3923 - accuracy: 0.4940 - val_loss: 1.3109 - val_accuracy: 0.5299\n",
      "Epoch 59/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3890 - accuracy: 0.4958\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.30275\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.30275\n",
      "\n",
      "Epoch 00059: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00059: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 12s 37ms/step - loss: 1.3893 - accuracy: 0.4957 - val_loss: 1.3047 - val_accuracy: 0.5297\n",
      "Epoch 60/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3866 - accuracy: 0.4987\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.30275\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.30275\n",
      "\n",
      "Epoch 00060: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00060: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00060: saving model to model_cifar10_cnn/KERAS_check_model_epoch60.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.3870 - accuracy: 0.4989 - val_loss: 1.3038 - val_accuracy: 0.5334\n",
      "Epoch 61/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3766 - accuracy: 0.5019\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.30275\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.30275\n",
      "\n",
      "Epoch 00061: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00061: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3767 - accuracy: 0.5020 - val_loss: 1.3796 - val_accuracy: 0.5126\n",
      "Epoch 62/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3839 - accuracy: 0.4956\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.30275\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.30275\n",
      "\n",
      "Epoch 00062: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00062: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3840 - accuracy: 0.4955 - val_loss: 1.3297 - val_accuracy: 0.5275\n",
      "Epoch 63/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3785 - accuracy: 0.5005\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00063: val_loss improved from 1.30275 to 1.30090, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00063: val_loss improved from 1.30275 to 1.30090, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00063: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00063: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3784 - accuracy: 0.5005 - val_loss: 1.3009 - val_accuracy: 0.5318\n",
      "Epoch 64/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3760 - accuracy: 0.5018\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00064: val_loss improved from 1.30090 to 1.29804, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00064: val_loss improved from 1.30090 to 1.29804, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00064: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00064: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3760 - accuracy: 0.5018 - val_loss: 1.2980 - val_accuracy: 0.5337\n",
      "Epoch 65/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - ETA: 0s - loss: 1.3761 - accuracy: 0.5005\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00065: val_loss improved from 1.29804 to 1.29303, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00065: val_loss improved from 1.29804 to 1.29303, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00065: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00065: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3761 - accuracy: 0.5005 - val_loss: 1.2930 - val_accuracy: 0.5395\n",
      "Epoch 66/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3749 - accuracy: 0.5032\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00066: val_loss improved from 1.29303 to 1.28670, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00066: val_loss improved from 1.29303 to 1.28670, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00066: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00066: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3749 - accuracy: 0.5032 - val_loss: 1.2867 - val_accuracy: 0.5378\n",
      "Epoch 67/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3716 - accuracy: 0.5032\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.28670\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.28670\n",
      "\n",
      "Epoch 00067: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00067: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3724 - accuracy: 0.5029 - val_loss: 1.2989 - val_accuracy: 0.5354\n",
      "Epoch 68/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3764 - accuracy: 0.5021\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.28670\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.28670\n",
      "\n",
      "Epoch 00068: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00068: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3764 - accuracy: 0.5021 - val_loss: 1.2981 - val_accuracy: 0.5379\n",
      "Epoch 69/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3745 - accuracy: 0.5038\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00069: val_loss improved from 1.28670 to 1.27840, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00069: val_loss improved from 1.28670 to 1.27840, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00069: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00069: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 37ms/step - loss: 1.3745 - accuracy: 0.5038 - val_loss: 1.2784 - val_accuracy: 0.5443\n",
      "Epoch 70/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3675 - accuracy: 0.5034\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.27840\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.27840\n",
      "\n",
      "Epoch 00070: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00070: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00070: saving model to model_cifar10_cnn/KERAS_check_model_epoch70.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 12s 37ms/step - loss: 1.3675 - accuracy: 0.5034 - val_loss: 1.3723 - val_accuracy: 0.5163\n",
      "Epoch 71/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3703 - accuracy: 0.5027\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.27840\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.27840\n",
      "\n",
      "Epoch 00071: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00071: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 36ms/step - loss: 1.3704 - accuracy: 0.5025 - val_loss: 1.2827 - val_accuracy: 0.5437\n",
      "Epoch 72/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3646 - accuracy: 0.5066\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00072: val_loss improved from 1.27840 to 1.27374, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00072: val_loss improved from 1.27840 to 1.27374, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00072: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00072: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 36ms/step - loss: 1.3646 - accuracy: 0.5066 - val_loss: 1.2737 - val_accuracy: 0.5476\n",
      "Epoch 73/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3611 - accuracy: 0.5038\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.27374\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.27374\n",
      "\n",
      "Epoch 00073: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00073: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 36ms/step - loss: 1.3611 - accuracy: 0.5038 - val_loss: 1.2958 - val_accuracy: 0.5379\n",
      "Epoch 74/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3619 - accuracy: 0.5049\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.27374\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.27374\n",
      "\n",
      "Epoch 00074: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00074: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.3619 - accuracy: 0.5049 - val_loss: 1.2761 - val_accuracy: 0.5439\n",
      "Epoch 75/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3621 - accuracy: 0.5045\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.27374\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.27374\n",
      "\n",
      "Epoch 00075: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00075: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.3621 - accuracy: 0.5043 - val_loss: 1.2876 - val_accuracy: 0.5384\n",
      "Epoch 76/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3575 - accuracy: 0.5109\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.27374\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.27374\n",
      "\n",
      "Epoch 00076: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00076: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3575 - accuracy: 0.5109 - val_loss: 1.2808 - val_accuracy: 0.5412\n",
      "Epoch 77/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3532 - accuracy: 0.5113\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.27374\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.27374\n",
      "\n",
      "Epoch 00077: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00077: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3532 - accuracy: 0.5113 - val_loss: 1.2805 - val_accuracy: 0.5409\n",
      "Epoch 78/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3564 - accuracy: 0.5078\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00078: val_loss improved from 1.27374 to 1.27222, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00078: val_loss improved from 1.27374 to 1.27222, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00078: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00078: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 36ms/step - loss: 1.3564 - accuracy: 0.5078 - val_loss: 1.2722 - val_accuracy: 0.5436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3617 - accuracy: 0.5053\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.27222\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.27222\n",
      "\n",
      "Epoch 00079: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00079: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 36ms/step - loss: 1.3617 - accuracy: 0.5053 - val_loss: 1.2742 - val_accuracy: 0.5417\n",
      "Epoch 80/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3528 - accuracy: 0.5108\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.27222\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.27222\n",
      "\n",
      "Epoch 00080: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00080: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00080: saving model to model_cifar10_cnn/KERAS_check_model_epoch80.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 37ms/step - loss: 1.3528 - accuracy: 0.5108 - val_loss: 1.2840 - val_accuracy: 0.5412\n",
      "Epoch 81/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3575 - accuracy: 0.5088\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.27222\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.27222\n",
      "\n",
      "Epoch 00081: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00081: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 36ms/step - loss: 1.3575 - accuracy: 0.5088 - val_loss: 1.2764 - val_accuracy: 0.5457\n",
      "Epoch 82/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3532 - accuracy: 0.5103\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.27222\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.27222\n",
      "\n",
      "Epoch 00082: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00082: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3537 - accuracy: 0.5104 - val_loss: 1.2761 - val_accuracy: 0.5422\n",
      "Epoch 83/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3601 - accuracy: 0.5099\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.27222\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.27222\n",
      "\n",
      "Epoch 00083: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00083: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3597 - accuracy: 0.5100 - val_loss: 1.3053 - val_accuracy: 0.5349\n",
      "Epoch 84/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3501 - accuracy: 0.5107\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.27222\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.27222\n",
      "\n",
      "Epoch 00084: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00084: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 1.3503 - accuracy: 0.5107 - val_loss: 1.2771 - val_accuracy: 0.5462\n",
      "Epoch 85/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3544 - accuracy: 0.5120\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00085: val_loss improved from 1.27222 to 1.25791, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00085: val_loss improved from 1.27222 to 1.25791, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00085: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00085: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3542 - accuracy: 0.5121 - val_loss: 1.2579 - val_accuracy: 0.5487\n",
      "Epoch 86/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3524 - accuracy: 0.5098\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.25791\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.25791\n",
      "\n",
      "Epoch 00086: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00086: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3519 - accuracy: 0.5101 - val_loss: 1.2760 - val_accuracy: 0.5431\n",
      "Epoch 87/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3506 - accuracy: 0.5142\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.25791\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.25791\n",
      "\n",
      "Epoch 00087: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00087: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 10s 34ms/step - loss: 1.3514 - accuracy: 0.5140 - val_loss: 1.2743 - val_accuracy: 0.5469\n",
      "Epoch 88/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3515 - accuracy: 0.5108\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1.25791\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1.25791\n",
      "\n",
      "Epoch 00088: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00088: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 1.3508 - accuracy: 0.5107 - val_loss: 1.2651 - val_accuracy: 0.5487\n",
      "Epoch 89/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3494 - accuracy: 0.5134\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.25791\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.25791\n",
      "\n",
      "Epoch 00089: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00089: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3494 - accuracy: 0.5134 - val_loss: 1.2618 - val_accuracy: 0.5490\n",
      "Epoch 90/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3453 - accuracy: 0.5141\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00090: val_loss improved from 1.25791 to 1.25691, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00090: val_loss improved from 1.25791 to 1.25691, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00090: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00090: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00090: saving model to model_cifar10_cnn/KERAS_check_model_epoch90.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3453 - accuracy: 0.5141 - val_loss: 1.2569 - val_accuracy: 0.5511\n",
      "Epoch 91/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3449 - accuracy: 0.5169\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.25691\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.25691\n",
      "\n",
      "Epoch 00091: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00091: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 1.3449 - accuracy: 0.5169 - val_loss: 1.2745 - val_accuracy: 0.5460\n",
      "Epoch 92/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3383 - accuracy: 0.5149\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.25691\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.25691\n",
      "\n",
      "Epoch 00092: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00092: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 10s 34ms/step - loss: 1.3383 - accuracy: 0.5149 - val_loss: 1.2652 - val_accuracy: 0.5470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3397 - accuracy: 0.5150\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00093: val_loss improved from 1.25691 to 1.24671, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00093: val_loss improved from 1.25691 to 1.24671, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00093: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00093: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3397 - accuracy: 0.5150 - val_loss: 1.2467 - val_accuracy: 0.5617\n",
      "Epoch 94/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3384 - accuracy: 0.5161\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1.24671\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1.24671\n",
      "\n",
      "Epoch 00094: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00094: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3385 - accuracy: 0.5163 - val_loss: 1.2603 - val_accuracy: 0.5553\n",
      "Epoch 95/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3353 - accuracy: 0.5144\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1.24671\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1.24671\n",
      "\n",
      "Epoch 00095: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00095: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3354 - accuracy: 0.5142 - val_loss: 1.2569 - val_accuracy: 0.5531\n",
      "Epoch 96/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3367 - accuracy: 0.5160\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.24671\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.24671\n",
      "\n",
      "Epoch 00096: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00096: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 1.3365 - accuracy: 0.5164 - val_loss: 1.2811 - val_accuracy: 0.5448\n",
      "Epoch 97/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3358 - accuracy: 0.5164\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.24671\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.24671\n",
      "\n",
      "Epoch 00097: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00097: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3358 - accuracy: 0.5166 - val_loss: 1.2503 - val_accuracy: 0.5573\n",
      "Epoch 98/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3374 - accuracy: 0.5170\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.24671\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.24671\n",
      "\n",
      "Epoch 00098: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00098: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3374 - accuracy: 0.5170 - val_loss: 1.2699 - val_accuracy: 0.5515\n",
      "Epoch 99/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3384 - accuracy: 0.5200\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.24671\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.24671\n",
      "\n",
      "Epoch 00099: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00099: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3393 - accuracy: 0.5197 - val_loss: 1.2719 - val_accuracy: 0.5467\n",
      "Epoch 100/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3357 - accuracy: 0.5171\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.24671\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.24671\n",
      "\n",
      "Epoch 00100: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00100: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00100: saving model to model_cifar10_cnn/KERAS_check_model_epoch100.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3357 - accuracy: 0.5171 - val_loss: 1.2481 - val_accuracy: 0.5557\n",
      "Epoch 101/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3324 - accuracy: 0.5183\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 1.24671\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 1.24671\n",
      "\n",
      "Epoch 00101: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00101: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3327 - accuracy: 0.5182 - val_loss: 1.2536 - val_accuracy: 0.5528\n",
      "Epoch 102/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3365 - accuracy: 0.5168\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 1.24671\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 1.24671\n",
      "\n",
      "Epoch 00102: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00102: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3363 - accuracy: 0.5169 - val_loss: 1.2996 - val_accuracy: 0.5435\n",
      "Epoch 103/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3362 - accuracy: 0.5160\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 1.24671\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 1.24671\n",
      "\n",
      "Epoch 00103: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00103: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3355 - accuracy: 0.5163 - val_loss: 1.2517 - val_accuracy: 0.5570\n",
      "Epoch 104/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3208 - accuracy: 0.5220\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 1.24671\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 1.24671\n",
      "\n",
      "Epoch 00104: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00104: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 1.3211 - accuracy: 0.5220 - val_loss: 1.2495 - val_accuracy: 0.5554\n",
      "Epoch 105/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3247 - accuracy: 0.5230\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00105: val_loss improved from 1.24671 to 1.24458, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00105: val_loss improved from 1.24671 to 1.24458, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00105: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00105: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 1.3248 - accuracy: 0.5231 - val_loss: 1.2446 - val_accuracy: 0.5562\n",
      "Epoch 106/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3242 - accuracy: 0.5217\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00106: val_loss improved from 1.24458 to 1.23851, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00106: val_loss improved from 1.24458 to 1.23851, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00106: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00106: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 1.3242 - accuracy: 0.5217 - val_loss: 1.2385 - val_accuracy: 0.5575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3225 - accuracy: 0.5215\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 1.23851\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 1.23851\n",
      "\n",
      "Epoch 00107: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00107: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3225 - accuracy: 0.5215 - val_loss: 1.2503 - val_accuracy: 0.5563\n",
      "Epoch 108/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3258 - accuracy: 0.5202\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 1.23851\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 1.23851\n",
      "\n",
      "Epoch 00108: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00108: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 10s 34ms/step - loss: 1.3258 - accuracy: 0.5201 - val_loss: 1.2388 - val_accuracy: 0.5603\n",
      "Epoch 109/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3228 - accuracy: 0.5219\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 1.23851\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 1.23851\n",
      "\n",
      "Epoch 00109: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00109: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 10s 34ms/step - loss: 1.3230 - accuracy: 0.5217 - val_loss: 1.2408 - val_accuracy: 0.5592\n",
      "Epoch 110/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3187 - accuracy: 0.5238\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 1.23851\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 1.23851\n",
      "\n",
      "Epoch 00110: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00110: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00110: saving model to model_cifar10_cnn/KERAS_check_model_epoch110.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3187 - accuracy: 0.5238 - val_loss: 1.2475 - val_accuracy: 0.5528\n",
      "Epoch 111/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3189 - accuracy: 0.5210\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 1.23851\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 1.23851\n",
      "\n",
      "Epoch 00111: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00111: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3192 - accuracy: 0.5210 - val_loss: 1.2419 - val_accuracy: 0.5608\n",
      "Epoch 112/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3164 - accuracy: 0.5234\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 1.23851\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 1.23851\n",
      "\n",
      "Epoch 00112: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00112: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 1.3164 - accuracy: 0.5234 - val_loss: 1.2497 - val_accuracy: 0.5564\n",
      "Epoch 113/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3190 - accuracy: 0.5226\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 1.23851\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 1.23851\n",
      "\n",
      "Epoch 00113: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00113: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3190 - accuracy: 0.5226 - val_loss: 1.2747 - val_accuracy: 0.5501\n",
      "Epoch 114/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3214 - accuracy: 0.5229\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 1.23851\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 1.23851\n",
      "\n",
      "Epoch 00114: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00114: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.3211 - accuracy: 0.5227 - val_loss: 1.2389 - val_accuracy: 0.5641\n",
      "Epoch 115/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3188 - accuracy: 0.5239\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00115: val_loss improved from 1.23851 to 1.23046, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00115: val_loss improved from 1.23851 to 1.23046, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00115: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00115: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3186 - accuracy: 0.5238 - val_loss: 1.2305 - val_accuracy: 0.5611\n",
      "Epoch 116/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3206 - accuracy: 0.5219\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 1.23046\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 1.23046\n",
      "\n",
      "Epoch 00116: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00116: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3206 - accuracy: 0.5219 - val_loss: 1.2516 - val_accuracy: 0.5567\n",
      "Epoch 117/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3160 - accuracy: 0.5240\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 1.23046\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 1.23046\n",
      "\n",
      "Epoch 00117: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00117: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3159 - accuracy: 0.5242 - val_loss: 1.2312 - val_accuracy: 0.5630\n",
      "Epoch 118/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3152 - accuracy: 0.5241\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 1.23046\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 1.23046\n",
      "\n",
      "Epoch 00118: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00118: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3151 - accuracy: 0.5240 - val_loss: 1.2533 - val_accuracy: 0.5514\n",
      "Epoch 119/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3168 - accuracy: 0.5278\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 1.23046\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 1.23046\n",
      "\n",
      "Epoch 00119: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00119: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3169 - accuracy: 0.5278 - val_loss: 1.2383 - val_accuracy: 0.5620\n",
      "Epoch 120/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3104 - accuracy: 0.5265\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 1.23046\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 1.23046\n",
      "\n",
      "Epoch 00120: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00120: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00120: saving model to model_cifar10_cnn/KERAS_check_model_epoch120.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3104 - accuracy: 0.5265 - val_loss: 1.2433 - val_accuracy: 0.5594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3163 - accuracy: 0.5236\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 1.23046\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 1.23046\n",
      "\n",
      "Epoch 00121: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00121: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3164 - accuracy: 0.5234 - val_loss: 1.2350 - val_accuracy: 0.5591\n",
      "Epoch 122/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3150 - accuracy: 0.5269\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 1.23046\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 1.23046\n",
      "\n",
      "Epoch 00122: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00122: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3150 - accuracy: 0.5269 - val_loss: 1.2307 - val_accuracy: 0.5623\n",
      "Epoch 123/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3230 - accuracy: 0.5189\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 1.23046\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 1.23046\n",
      "\n",
      "Epoch 00123: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00123: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3230 - accuracy: 0.5189 - val_loss: 1.2348 - val_accuracy: 0.5665\n",
      "Epoch 124/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3097 - accuracy: 0.5248\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00124: val_loss improved from 1.23046 to 1.22686, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00124: val_loss improved from 1.23046 to 1.22686, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00124: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00124: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3095 - accuracy: 0.5251 - val_loss: 1.2269 - val_accuracy: 0.5654\n",
      "Epoch 125/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3128 - accuracy: 0.5245\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00125: val_loss improved from 1.22686 to 1.22603, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00125: val_loss improved from 1.22686 to 1.22603, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00125: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00125: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.3123 - accuracy: 0.5246 - val_loss: 1.2260 - val_accuracy: 0.5672\n",
      "Epoch 126/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3100 - accuracy: 0.5276\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00126: val_loss improved from 1.22603 to 1.22513, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00126: val_loss improved from 1.22603 to 1.22513, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00126: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00126: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.3099 - accuracy: 0.5276 - val_loss: 1.2251 - val_accuracy: 0.5664\n",
      "Epoch 127/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3045 - accuracy: 0.5279\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00127: val_loss improved from 1.22513 to 1.22420, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00127: val_loss improved from 1.22513 to 1.22420, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00127: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00127: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3048 - accuracy: 0.5276 - val_loss: 1.2242 - val_accuracy: 0.5677\n",
      "Epoch 128/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3099 - accuracy: 0.5294\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 1.22420\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 1.22420\n",
      "\n",
      "Epoch 00128: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00128: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3099 - accuracy: 0.5294 - val_loss: 1.2274 - val_accuracy: 0.5661\n",
      "Epoch 129/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3086 - accuracy: 0.5288\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00129: val_loss improved from 1.22420 to 1.22103, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00129: val_loss improved from 1.22420 to 1.22103, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00129: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00129: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3094 - accuracy: 0.5285 - val_loss: 1.2210 - val_accuracy: 0.5668\n",
      "Epoch 130/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3060 - accuracy: 0.5264\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 1.22103\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 1.22103\n",
      "\n",
      "Epoch 00130: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00130: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00130: saving model to model_cifar10_cnn/KERAS_check_model_epoch130.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3060 - accuracy: 0.5264 - val_loss: 1.2316 - val_accuracy: 0.5665\n",
      "Epoch 131/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3038 - accuracy: 0.5299\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 1.22103\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 1.22103\n",
      "\n",
      "Epoch 00131: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00131: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3038 - accuracy: 0.5299 - val_loss: 1.2665 - val_accuracy: 0.5539\n",
      "Epoch 132/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3063 - accuracy: 0.5294\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 1.22103\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 1.22103\n",
      "\n",
      "Epoch 00132: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00132: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3070 - accuracy: 0.5292 - val_loss: 1.2250 - val_accuracy: 0.5661\n",
      "Epoch 133/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3119 - accuracy: 0.5273\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 1.22103\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 1.22103\n",
      "\n",
      "Epoch 00133: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00133: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3116 - accuracy: 0.5275 - val_loss: 1.2280 - val_accuracy: 0.5633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3041 - accuracy: 0.5320\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00134: val_loss improved from 1.22103 to 1.21807, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00134: val_loss improved from 1.22103 to 1.21807, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00134: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00134: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3041 - accuracy: 0.5320 - val_loss: 1.2181 - val_accuracy: 0.5652\n",
      "Epoch 135/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3071 - accuracy: 0.5282\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 1.21807\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 1.21807\n",
      "\n",
      "Epoch 00135: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00135: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3071 - accuracy: 0.5282 - val_loss: 1.2309 - val_accuracy: 0.5608\n",
      "Epoch 136/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3169 - accuracy: 0.5263\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 1.21807\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 1.21807\n",
      "\n",
      "Epoch 00136: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00136: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3169 - accuracy: 0.5263 - val_loss: 1.2284 - val_accuracy: 0.5647\n",
      "Epoch 137/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3082 - accuracy: 0.5247\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00137: val_loss improved from 1.21807 to 1.21685, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00137: val_loss improved from 1.21807 to 1.21685, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00137: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00137: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3086 - accuracy: 0.5244 - val_loss: 1.2169 - val_accuracy: 0.5688\n",
      "Epoch 138/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3041 - accuracy: 0.5274\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 1.21685\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 1.21685\n",
      "\n",
      "Epoch 00138: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00138: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3035 - accuracy: 0.5278 - val_loss: 1.2369 - val_accuracy: 0.5607\n",
      "Epoch 139/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3120 - accuracy: 0.5271\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 1.21685\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 1.21685\n",
      "\n",
      "Epoch 00139: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00139: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3119 - accuracy: 0.5271 - val_loss: 1.2211 - val_accuracy: 0.5673\n",
      "Epoch 140/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3120 - accuracy: 0.5264\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 1.21685\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 1.21685\n",
      "\n",
      "Epoch 00140: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00140: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00140: saving model to model_cifar10_cnn/KERAS_check_model_epoch140.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3120 - accuracy: 0.5264 - val_loss: 1.2422 - val_accuracy: 0.5589\n",
      "Epoch 141/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3017 - accuracy: 0.5319\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 1.21685\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 1.21685\n",
      "\n",
      "Epoch 00141: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00141: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3017 - accuracy: 0.5319 - val_loss: 1.2205 - val_accuracy: 0.5674\n",
      "Epoch 142/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3019 - accuracy: 0.5341\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 1.21685\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 1.21685\n",
      "\n",
      "Epoch 00142: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00142: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3023 - accuracy: 0.5337 - val_loss: 1.2323 - val_accuracy: 0.5674\n",
      "Epoch 143/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2903 - accuracy: 0.5355\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 1.21685\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 1.21685\n",
      "\n",
      "Epoch 00143: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00143: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2903 - accuracy: 0.5355 - val_loss: 1.2169 - val_accuracy: 0.5658\n",
      "Epoch 144/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.2997 - accuracy: 0.5294\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00144: val_loss improved from 1.21685 to 1.21310, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00144: val_loss improved from 1.21685 to 1.21310, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00144: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00144: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2997 - accuracy: 0.5295 - val_loss: 1.2131 - val_accuracy: 0.5724\n",
      "Epoch 145/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3029 - accuracy: 0.5298\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 1.21310\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 1.21310\n",
      "\n",
      "Epoch 00145: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00145: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3033 - accuracy: 0.5294 - val_loss: 1.2225 - val_accuracy: 0.5659\n",
      "Epoch 146/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3032 - accuracy: 0.5308\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 1.21310\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 1.21310\n",
      "\n",
      "Epoch 00146: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00146: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.3032 - accuracy: 0.5308 - val_loss: 1.2393 - val_accuracy: 0.5631\n",
      "Epoch 147/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2976 - accuracy: 0.5327\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 1.21310\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 1.21310\n",
      "\n",
      "Epoch 00147: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00147: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2976 - accuracy: 0.5327 - val_loss: 1.2161 - val_accuracy: 0.5720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.2945 - accuracy: 0.5330\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00148: val_loss improved from 1.21310 to 1.21179, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00148: val_loss improved from 1.21310 to 1.21179, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00148: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00148: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2948 - accuracy: 0.5328 - val_loss: 1.2118 - val_accuracy: 0.5732\n",
      "Epoch 149/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3028 - accuracy: 0.5322\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00149: val_loss improved from 1.21179 to 1.20964, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00149: val_loss improved from 1.21179 to 1.20964, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00149: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00149: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3021 - accuracy: 0.5326 - val_loss: 1.2096 - val_accuracy: 0.5733\n",
      "Epoch 150/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3004 - accuracy: 0.5294\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 1.20964\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 1.20964\n",
      "\n",
      "Epoch 00150: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00150: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00150: saving model to model_cifar10_cnn/KERAS_check_model_epoch150.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.3004 - accuracy: 0.5294 - val_loss: 1.2237 - val_accuracy: 0.5628\n",
      "Epoch 151/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.3040 - accuracy: 0.5325\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 1.20964\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 1.20964\n",
      "\n",
      "Epoch 00151: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00151: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3040 - accuracy: 0.5325 - val_loss: 1.2175 - val_accuracy: 0.5692\n",
      "Epoch 152/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.2977 - accuracy: 0.5314\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 1.20964\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 1.20964\n",
      "\n",
      "Epoch 00152: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00152: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2978 - accuracy: 0.5316 - val_loss: 1.2234 - val_accuracy: 0.5668\n",
      "Epoch 153/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.2990 - accuracy: 0.5326\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 1.20964\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 1.20964\n",
      "\n",
      "Epoch 00153: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00153: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2991 - accuracy: 0.5328 - val_loss: 1.2265 - val_accuracy: 0.5679\n",
      "Epoch 154/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.3008 - accuracy: 0.5298\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 1.20964\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 1.20964\n",
      "\n",
      "Epoch 00154: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00154: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3007 - accuracy: 0.5299 - val_loss: 1.2099 - val_accuracy: 0.5687\n",
      "Epoch 155/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.2998 - accuracy: 0.5304\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 1.20964\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 1.20964\n",
      "\n",
      "Epoch 00155: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00155: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.3001 - accuracy: 0.5302 - val_loss: 1.2239 - val_accuracy: 0.5680\n",
      "Epoch 156/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2964 - accuracy: 0.5352\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 1.20964\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 1.20964\n",
      "\n",
      "Epoch 00156: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00156: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2964 - accuracy: 0.5352 - val_loss: 1.2526 - val_accuracy: 0.5582\n",
      "Epoch 157/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.2974 - accuracy: 0.5346\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 1.20964\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 1.20964\n",
      "\n",
      "Epoch 00157: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00157: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2971 - accuracy: 0.5346 - val_loss: 1.2118 - val_accuracy: 0.5687\n",
      "Epoch 158/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2923 - accuracy: 0.5335\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 1.20964\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 1.20964\n",
      "\n",
      "Epoch 00158: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00158: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2923 - accuracy: 0.5335 - val_loss: 1.2157 - val_accuracy: 0.5719\n",
      "Epoch 159/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.2974 - accuracy: 0.5333\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 1.20964\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 1.20964\n",
      "\n",
      "Epoch 00159: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00159: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00159: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2967 - accuracy: 0.5336 - val_loss: 1.2211 - val_accuracy: 0.5718\n",
      "Epoch 160/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2907 - accuracy: 0.5360\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 1.20964\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 1.20964\n",
      "\n",
      "Epoch 00160: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00160: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00160: saving model to model_cifar10_cnn/KERAS_check_model_epoch160.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2907 - accuracy: 0.5360 - val_loss: 1.2168 - val_accuracy: 0.5657\n",
      "Epoch 161/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.2890 - accuracy: 0.5377\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 1.20964\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 1.20964\n",
      "\n",
      "Epoch 00161: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00161: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.2895 - accuracy: 0.5377 - val_loss: 1.2096 - val_accuracy: 0.5721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2926 - accuracy: 0.5351\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00162: val_loss improved from 1.20964 to 1.20637, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00162: val_loss improved from 1.20964 to 1.20637, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00162: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00162: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2926 - accuracy: 0.5351 - val_loss: 1.2064 - val_accuracy: 0.5715\n",
      "Epoch 163/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2928 - accuracy: 0.5345\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00163: val_loss improved from 1.20637 to 1.20228, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00163: val_loss improved from 1.20637 to 1.20228, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00163: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00163: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2928 - accuracy: 0.5345 - val_loss: 1.2023 - val_accuracy: 0.5745\n",
      "Epoch 164/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2883 - accuracy: 0.5352\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 1.20228\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 1.20228\n",
      "\n",
      "Epoch 00164: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00164: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2883 - accuracy: 0.5352 - val_loss: 1.2304 - val_accuracy: 0.5673\n",
      "Epoch 165/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2941 - accuracy: 0.5361\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 1.20228\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 1.20228\n",
      "\n",
      "Epoch 00165: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00165: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.2941 - accuracy: 0.5361 - val_loss: 1.2059 - val_accuracy: 0.5754\n",
      "Epoch 166/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2936 - accuracy: 0.5343\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 1.20228\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 1.20228\n",
      "\n",
      "Epoch 00166: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00166: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.2936 - accuracy: 0.5343 - val_loss: 1.2050 - val_accuracy: 0.5766\n",
      "Epoch 167/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2896 - accuracy: 0.5319\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 1.20228\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 1.20228\n",
      "\n",
      "Epoch 00167: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00167: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.2896 - accuracy: 0.5319 - val_loss: 1.2043 - val_accuracy: 0.5755\n",
      "Epoch 168/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2905 - accuracy: 0.5366\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 1.20228\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 1.20228\n",
      "\n",
      "Epoch 00168: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00168: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.2905 - accuracy: 0.5366 - val_loss: 1.2044 - val_accuracy: 0.5765\n",
      "Epoch 169/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2934 - accuracy: 0.5335\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 1.20228\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 1.20228\n",
      "\n",
      "Epoch 00169: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00169: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.2934 - accuracy: 0.5335 - val_loss: 1.2052 - val_accuracy: 0.5713\n",
      "Epoch 170/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.2872 - accuracy: 0.5367\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00170: val_loss improved from 1.20228 to 1.20221, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00170: val_loss improved from 1.20228 to 1.20221, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00170: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00170: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00170: saving model to model_cifar10_cnn/KERAS_check_model_epoch170.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.2874 - accuracy: 0.5367 - val_loss: 1.2022 - val_accuracy: 0.5760\n",
      "Epoch 171/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2895 - accuracy: 0.5334\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 1.20221\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 1.20221\n",
      "\n",
      "Epoch 00171: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00171: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.2895 - accuracy: 0.5334 - val_loss: 1.2057 - val_accuracy: 0.5771\n",
      "Epoch 172/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2927 - accuracy: 0.5324\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 1.20221\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 1.20221\n",
      "\n",
      "Epoch 00172: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00172: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.2927 - accuracy: 0.5324 - val_loss: 1.2117 - val_accuracy: 0.5725\n",
      "Epoch 173/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2906 - accuracy: 0.5350\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 1.20221\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 1.20221\n",
      "\n",
      "Epoch 00173: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00173: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2906 - accuracy: 0.5350 - val_loss: 1.2101 - val_accuracy: 0.5728\n",
      "Epoch 174/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.2887 - accuracy: 0.5355\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00174: val_loss improved from 1.20221 to 1.20169, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00174: val_loss improved from 1.20221 to 1.20169, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00174: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00174: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2886 - accuracy: 0.5355 - val_loss: 1.2017 - val_accuracy: 0.5695\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - ETA: 0s - loss: 1.2896 - accuracy: 0.5319\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 1.20169\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 1.20169\n",
      "\n",
      "Epoch 00175: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00175: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2896 - accuracy: 0.5319 - val_loss: 1.2253 - val_accuracy: 0.5629\n",
      "Epoch 176/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.2886 - accuracy: 0.5358\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 1.20169\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 1.20169\n",
      "\n",
      "Epoch 00176: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00176: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2886 - accuracy: 0.5360 - val_loss: 1.2116 - val_accuracy: 0.5722\n",
      "Epoch 177/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2913 - accuracy: 0.5306\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 1.20169\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 1.20169\n",
      "\n",
      "Epoch 00177: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00177: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2913 - accuracy: 0.5306 - val_loss: 1.2127 - val_accuracy: 0.5666\n",
      "Epoch 178/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.2908 - accuracy: 0.5362\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 1.20169\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 1.20169\n",
      "\n",
      "Epoch 00178: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00178: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2907 - accuracy: 0.5362 - val_loss: 1.2061 - val_accuracy: 0.5730\n",
      "Epoch 179/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2890 - accuracy: 0.5358\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 1.20169\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 1.20169\n",
      "\n",
      "Epoch 00179: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00179: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2890 - accuracy: 0.5358 - val_loss: 1.2150 - val_accuracy: 0.5728\n",
      "Epoch 180/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2862 - accuracy: 0.5367\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 1.20169\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 1.20169\n",
      "\n",
      "Epoch 00180: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00180: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00180: saving model to model_cifar10_cnn/KERAS_check_model_epoch180.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2862 - accuracy: 0.5367 - val_loss: 1.2049 - val_accuracy: 0.5776\n",
      "Epoch 181/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2850 - accuracy: 0.5379\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 1.20169\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 1.20169\n",
      "\n",
      "Epoch 00181: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00181: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2850 - accuracy: 0.5379 - val_loss: 1.2096 - val_accuracy: 0.5763\n",
      "Epoch 182/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.2872 - accuracy: 0.5331\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 1.20169\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 1.20169\n",
      "\n",
      "Epoch 00182: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00182: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2878 - accuracy: 0.5329 - val_loss: 1.2029 - val_accuracy: 0.5742\n",
      "Epoch 183/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.2883 - accuracy: 0.5352\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00183: val_loss improved from 1.20169 to 1.20018, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00183: val_loss improved from 1.20169 to 1.20018, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00183: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00183: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2882 - accuracy: 0.5351 - val_loss: 1.2002 - val_accuracy: 0.5770\n",
      "Epoch 184/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2897 - accuracy: 0.5364\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00184: val_loss improved from 1.20018 to 1.19777, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00184: val_loss improved from 1.20018 to 1.19777, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00184: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00184: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2897 - accuracy: 0.5364 - val_loss: 1.1978 - val_accuracy: 0.5778\n",
      "Epoch 185/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2856 - accuracy: 0.5365\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 1.19777\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 1.19777\n",
      "\n",
      "Epoch 00185: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00185: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2856 - accuracy: 0.5365 - val_loss: 1.2027 - val_accuracy: 0.5710\n",
      "Epoch 186/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2844 - accuracy: 0.5386\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 1.19777\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 1.19777\n",
      "\n",
      "Epoch 00186: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00186: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2844 - accuracy: 0.5386 - val_loss: 1.2038 - val_accuracy: 0.5730\n",
      "Epoch 187/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2814 - accuracy: 0.5406\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 1.19777\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 1.19777\n",
      "\n",
      "Epoch 00187: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00187: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2814 - accuracy: 0.5406 - val_loss: 1.2055 - val_accuracy: 0.5720\n",
      "Epoch 188/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.2838 - accuracy: 0.5365\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 1.19777\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 1.19777\n",
      "\n",
      "Epoch 00188: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00188: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2836 - accuracy: 0.5367 - val_loss: 1.2038 - val_accuracy: 0.5743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2878 - accuracy: 0.5345\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 1.19777\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 1.19777\n",
      "\n",
      "Epoch 00189: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00189: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2878 - accuracy: 0.5345 - val_loss: 1.2151 - val_accuracy: 0.5694\n",
      "Epoch 190/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.2800 - accuracy: 0.5374\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 1.19777\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 1.19777\n",
      "\n",
      "Epoch 00190: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00190: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00190: saving model to model_cifar10_cnn/KERAS_check_model_epoch190.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2799 - accuracy: 0.5375 - val_loss: 1.2019 - val_accuracy: 0.5758\n",
      "Epoch 191/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2865 - accuracy: 0.5352\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 1.19777\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 1.19777\n",
      "\n",
      "Epoch 00191: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00191: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2865 - accuracy: 0.5352 - val_loss: 1.2033 - val_accuracy: 0.5758\n",
      "Epoch 192/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2811 - accuracy: 0.5405\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 1.19777\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 1.19777\n",
      "\n",
      "Epoch 00192: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00192: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2811 - accuracy: 0.5405 - val_loss: 1.2044 - val_accuracy: 0.5773\n",
      "Epoch 193/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2801 - accuracy: 0.5382\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00193: val_loss improved from 1.19777 to 1.19571, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00193: val_loss improved from 1.19777 to 1.19571, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00193: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00193: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.2801 - accuracy: 0.5382 - val_loss: 1.1957 - val_accuracy: 0.5777\n",
      "Epoch 194/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2817 - accuracy: 0.5372\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 1.19571\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 1.19571\n",
      "\n",
      "Epoch 00194: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00194: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2817 - accuracy: 0.5372 - val_loss: 1.2121 - val_accuracy: 0.5702\n",
      "Epoch 195/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2839 - accuracy: 0.5364\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 1.19571\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 1.19571\n",
      "\n",
      "Epoch 00195: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00195: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 35ms/step - loss: 1.2839 - accuracy: 0.5364 - val_loss: 1.2077 - val_accuracy: 0.5759\n",
      "Epoch 196/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.2843 - accuracy: 0.5367\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 1.19571\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 1.19571\n",
      "\n",
      "Epoch 00196: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00196: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2844 - accuracy: 0.5368 - val_loss: 1.1967 - val_accuracy: 0.5790\n",
      "Epoch 197/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2833 - accuracy: 0.5387\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 1.19571\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 1.19571\n",
      "\n",
      "Epoch 00197: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00197: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2833 - accuracy: 0.5387 - val_loss: 1.2233 - val_accuracy: 0.5699\n",
      "Epoch 198/200\n",
      "313/313 [==============================] - ETA: 0s - loss: 1.2839 - accuracy: 0.5377\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 1.19571\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 1.19571\n",
      "\n",
      "Epoch 00198: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00198: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2839 - accuracy: 0.5377 - val_loss: 1.2006 - val_accuracy: 0.5786\n",
      "Epoch 199/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.2802 - accuracy: 0.5378\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00199: val_loss improved from 1.19571 to 1.19186, saving model to model_cifar10_cnn/KERAS_check_best_model.h5\n",
      "\n",
      "Epoch 00199: val_loss improved from 1.19571 to 1.19186, saving model to model_cifar10_cnn/KERAS_check_best_model_weights.h5\n",
      "\n",
      "Epoch 00199: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00199: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2802 - accuracy: 0.5379 - val_loss: 1.1919 - val_accuracy: 0.5793\n",
      "Epoch 200/200\n",
      "311/313 [============================>.] - ETA: 0s - loss: 1.2776 - accuracy: 0.5410\n",
      "***callbacks***\n",
      "saving losses to model_cifar10_cnn/losses.log\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 1.19186\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 1.19186\n",
      "\n",
      "Epoch 00200: saving model to model_cifar10_cnn/KERAS_check_model_last.h5\n",
      "\n",
      "Epoch 00200: saving model to model_cifar10_cnn/KERAS_check_model_last_weights.h5\n",
      "\n",
      "Epoch 00200: saving model to model_cifar10_cnn/KERAS_check_model_epoch200.h5\n",
      "\n",
      "***callbacks end***\n",
      "\n",
      "313/313 [==============================] - 11s 34ms/step - loss: 1.2773 - accuracy: 0.5411 - val_loss: 1.2021 - val_accuracy: 0.5737\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from callbacks import all_callbacks\n",
    "\n",
    "train = True\n",
    "\n",
    "\n",
    "if train:\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "    callbacks = all_callbacks(stop_patience = 1000,\n",
    "                              lr_factor = 0.5,\n",
    "                              lr_patience = 10,\n",
    "                              lr_epsilon = 0.000001,\n",
    "                              lr_cooldown = 2,\n",
    "                              lr_minimum = 0.0000001,\n",
    "                              outputDir = 'model_cifar10_cnn')\n",
    "    callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "    model.fit(x_train, y_train, batch_size=128,\n",
    "              epochs=200, validation_split=0.2, shuffle=True,\n",
    "              callbacks = callbacks.callbacks)\n",
    "    model = strip_pruning(model)\n",
    "    model.save('model_cifar10_cnn/KERAS_check_best_model.h5')\n",
    "else:\n",
    "    from qkeras.utils import load_qmodel\n",
    "    model = load_qmodel('model_cifar10_cnn/KERAS_check_best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfb0d84",
   "metadata": {},
   "source": [
    "## Check performance\n",
    "How does this model which was trained using 6-bits, and 75% sparsity model compare against the original model? Let's report the accuracy and make a ROC curve. The quantized, pruned model is shown with solid lines, the unpruned model from part 1 is shown with dashed lines.\n",
    "\n",
    "\n",
    "We should also check that hls4ml can respect the choice to use 6-bits throughout the model, and match the accuracy. We'll generate a configuration from this Quantized model, and plot its performance as the dotted line.\n",
    "The generated configuration is printed out. You'll notice that it uses 7 bits for the type, but we specified 6!? That's just because QKeras doesn't count the sign-bit when we specify the number of bits, so the type that actually gets used needs 1 more.\n",
    "\n",
    "We also use the `OutputRoundingSaturationMode` optimizer pass of `hls4ml` to set the Activation layers to round, rather than truncate, the cast. This is important for getting good model accuracy when using small bit precision activations. And we'll set a different data type for the tables used in the Softmax, just for a bit of extra performance.\n",
    "\n",
    "\n",
    "**Make sure you've trained the model from part 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b764db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: Input\n",
      "Layer name: q_conv2d, layer type: QConv2D\n",
      "Layer name: q_activation, layer type: QActivation\n",
      "Layer name: q_conv2d_1, layer type: QConv2D\n",
      "Layer name: q_activation_1, layer type: QActivation\n",
      "Layer name: max_pooling2d, layer type: MaxPooling2D\n",
      "Layer name: q_conv2d_2, layer type: QConv2D\n",
      "Layer name: q_activation_2, layer type: QActivation\n",
      "Layer name: q_conv2d_3, layer type: QConv2D\n",
      "Layer name: q_activation_3, layer type: QActivation\n",
      "Layer name: max_pooling2d_1, layer type: MaxPooling2D\n",
      "Layer name: q_dense, layer type: QDense\n",
      "Layer name: activation, layer type: Activation\n",
      "-----------------------------------\n",
      "Configuration\n",
      "Backend:             VivadoAccelerator\n",
      "OutputDir:           cifar10-hls-test-1\n",
      "ProjectName:         myproject_cifar10_cnn4\n",
      "XilinxPart:          xczu7ev-ffvc1156-2-e\n",
      "Board:               zcu104\n",
      "ClockPeriod:         5\n",
      "IOType:              io_stream\n",
      "HLSConfig\n",
      "  Model\n",
      "    Precision:       ap_fixed<16,6>\n",
      "    ReuseFactor:     1\n",
      "    Strategy:        Latency\n",
      "  LayerName\n",
      "    input_1\n",
      "      Precision\n",
      "        result:      ap_fixed<16,6>\n",
      "    q_conv2d\n",
      "      Precision\n",
      "        weight:      ap_fixed<6,1>\n",
      "        bias:        ap_fixed<6,1>\n",
      "      ReuseFactor:   1\n",
      "    q_activation\n",
      "      Precision\n",
      "        result:      ap_fixed<6,1>\n",
      "      ReuseFactor:   1\n",
      "    q_conv2d_1\n",
      "      Precision\n",
      "        weight:      ap_fixed<6,1>\n",
      "        bias:        ap_fixed<6,1>\n",
      "      ReuseFactor:   1\n",
      "    q_activation_1\n",
      "      Precision\n",
      "        result:      ap_fixed<6,1>\n",
      "      ReuseFactor:   1\n",
      "    max_pooling2d\n",
      "      Precision:     ap_fixed<16,6>\n",
      "    q_conv2d_2\n",
      "      Precision\n",
      "        weight:      ap_fixed<6,1>\n",
      "        bias:        ap_fixed<6,1>\n",
      "      ReuseFactor:   1\n",
      "    q_activation_2\n",
      "      Precision\n",
      "        result:      ap_fixed<6,1>\n",
      "      ReuseFactor:   1\n",
      "    q_conv2d_3\n",
      "      Precision\n",
      "        weight:      ap_fixed<6,1>\n",
      "        bias:        ap_fixed<6,1>\n",
      "      ReuseFactor:   1\n",
      "    q_activation_3\n",
      "      Precision\n",
      "        result:      ap_fixed<6,1>\n",
      "      ReuseFactor:   1\n",
      "    max_pooling2d_1\n",
      "      Precision:     ap_fixed<16,6>\n",
      "    q_dense\n",
      "      Precision\n",
      "        weight:      ap_fixed<6,1>\n",
      "        bias:        ap_fixed<6,1>\n",
      "      ReuseFactor:   1\n",
      "    activation\n",
      "      Precision:     ap_fixed<16,6>\n",
      "      ReuseFactor:   1\n",
      "      table_size:    1024\n",
      "      exp_table_t:   ap_fixed<18,8,AP_RND,AP_SAT>\n",
      "      inv_table_t:   ap_fixed<18,8,AP_RND,AP_SAT>\n",
      "AcceleratorConfig\n",
      "  Interface:         axi_stream\n",
      "  Driver:            python\n",
      "  Precision\n",
      "    Input:           float\n",
      "    Output:          float\n",
      "KerasModel:          <tensorflow.python.keras.engine.sequential.Sequential object at 0x7f89381f0e10>\n",
      "-----------------------------------\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_1, layer type: InputLayer, input shapes: [[None, 32, 32, 3]], output shape: [None, 32, 32, 3]\n",
      "Layer name: q_conv2d, layer type: QConv2D, input shapes: [[None, 32, 32, 3]], output shape: [None, 30, 30, 16]\n",
      "Layer name: q_activation, layer type: Activation, input shapes: [[None, 30, 30, 16]], output shape: [None, 30, 30, 16]\n",
      "Layer name: q_conv2d_1, layer type: QConv2D, input shapes: [[None, 30, 30, 16]], output shape: [None, 28, 28, 16]\n",
      "Layer name: q_activation_1, layer type: Activation, input shapes: [[None, 28, 28, 16]], output shape: [None, 28, 28, 16]\n",
      "Layer name: max_pooling2d, layer type: MaxPooling2D, input shapes: [[None, 28, 28, 16]], output shape: [None, 14, 14, 16]\n",
      "Layer name: q_conv2d_2, layer type: QConv2D, input shapes: [[None, 14, 14, 16]], output shape: [None, 12, 12, 8]\n",
      "Layer name: q_activation_2, layer type: Activation, input shapes: [[None, 12, 12, 8]], output shape: [None, 12, 12, 8]\n",
      "Layer name: q_conv2d_3, layer type: QConv2D, input shapes: [[None, 12, 12, 8]], output shape: [None, 10, 10, 8]\n",
      "Layer name: q_activation_3, layer type: Activation, input shapes: [[None, 10, 10, 8]], output shape: [None, 10, 10, 8]\n",
      "Layer name: max_pooling2d_1, layer type: MaxPooling2D, input shapes: [[None, 10, 10, 8]], output shape: [None, 5, 5, 8]\n",
      "Layer name: flatten, layer type: Reshape, input shapes: [[None, 5, 5, 8]], output shape: [None, 200]\n",
      "Layer name: q_dense, layer type: QDense, input shapes: [[None, 200]], output shape: [None, 10]\n",
      "Layer name: activation, layer type: Softmax, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Creating HLS model\n",
      "Writing HLS project\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import hls4ml\n",
    "from hls4ml.converters.keras_to_hls import keras_to_hls\n",
    "import plotting\n",
    "import yaml\n",
    "\n",
    "hls4ml.model.optimizer.OutputRoundingSaturationMode.layers = ['Activation']\n",
    "hls4ml.model.optimizer.OutputRoundingSaturationMode.rounding_mode = 'AP_RND'\n",
    "hls4ml.model.optimizer.OutputRoundingSaturationMode.saturation_mode = 'AP_SAT'\n",
    "\n",
    "\n",
    "\n",
    "config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "config['Backend']='VivadoAccelerator'\n",
    "config['OutputDir'] = 'cifar10-hls-test-1'\n",
    "config['ProjectName'] = 'myproject_cifar10_cnn4'\n",
    "config['XilinxPart']= 'xczu7ev-ffvc1156-2-e'\n",
    "config['Board'] = 'zcu104'\n",
    "config['ClockPeriod'] = 5\n",
    "config['IOType'] = 'io_stream'\n",
    "config['HLSConfig']={}\n",
    "config['HLSConfig']['Model']={}\n",
    "config['HLSConfig']['Model']=config['Model']\n",
    "config['HLSConfig']['LayerName']=config['LayerName']\n",
    "\n",
    "del config['Model']\n",
    "del config['LayerName']\n",
    "config['AcceleratorConfig']={}\n",
    "config['AcceleratorConfig']['Interface'] = 'axi_stream'\n",
    "config['AcceleratorConfig']['Driver'] = 'python'\n",
    "config['AcceleratorConfig']['Precision']={}\n",
    "config['AcceleratorConfig']['Precision']['Input']= 'float'\n",
    "config['AcceleratorConfig']['Precision']['Output']= 'float'\n",
    "config['KerasModel'] = model\n",
    "\n",
    "\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Configuration\")\n",
    "plotting.print_dict(config)\n",
    "print(\"-----------------------------------\")\n",
    "hls_model = keras_to_hls(config)\n",
    "hls_model.compile()\n",
    "x_test = np.ascontiguousarray(x_test)  #add more\n",
    "y_qkeras = model.predict(x_test)\n",
    "y_hls = hls_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b5b8164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy baseline:  0.5798\n",
      "Accuracy pruned, quantized: 0.5798\n",
      "Accuracy hls4ml: 0.5745\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "from qkeras.utils import load_qmodel\n",
    "\n",
    "model_ref = load_qmodel('model_cifar10_cnn/KERAS_check_best_model.h5')\n",
    "y_ref = model_ref.predict(x_test)\n",
    "\n",
    "print(\"Accuracy baseline:  {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_ref, axis=1))))\n",
    "print(\"Accuracy pruned, quantized: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_qkeras, axis=1))))\n",
    "print(\"Accuracy hls4ml: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_hls, axis=1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ee25da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f898ee4d990>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "mnist_classes=['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 9))\n",
    "_ = plotting.makeRoc(y_test, y_ref, mnist_classes)\n",
    "plt.gca().set_prop_cycle(None) # reset the colors\n",
    "_ = plotting.makeRoc(y_test, y_qkeras, mnist_classes, linestyle='--')\n",
    "plt.gca().set_prop_cycle(None) # reset the colors\n",
    "_ = plotting.makeRoc(y_test, y_hls,mnist_classes, linestyle=':')\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "lines = [Line2D([0], [0], ls='-'),\n",
    "         Line2D([0], [0], ls='--'),\n",
    "         Line2D([0], [0], ls=':')]\n",
    "from matplotlib.legend import Legend\n",
    "leg = Legend(ax, lines, labels=['baseline', 'pruned, quantized', 'hls4ml'],\n",
    "            loc='lower left', frameon=False)\n",
    "ax.add_artist(leg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b174a2f",
   "metadata": {},
   "source": [
    "# Synthesize\n",
    "Now let's synthesize this quantized, pruned model.\n",
    "\n",
    "**The synthesis will take a while**\n",
    "\n",
    "While the C-Synthesis is running, we can monitor the progress looking at the log file by opening a terminal from the notebook home, and executing:\n",
    "\n",
    "`tail -f mnist-hls-test4/vivado_hls.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57f155a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)\n",
      "  **** SW Build 2708876 on Wed Nov  6 21:39:14 MST 2019\n",
      "  **** IP Build 2700528 on Thu Nov  7 00:09:20 MST 2019\n",
      "    ** Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.\n",
      "\n",
      "source /workspace/home/Xilinx/Vivado/2019.2/scripts/vivado_hls/hls.tcl -notrace\n",
      "INFO: [HLS 200-10] Running '/workspace/home/Xilinx/Vivado/2019.2/bin/unwrapped/lnx64.o/vivado_hls'\n",
      "/workspace/home/Xilinx/Vivado/2019.2/tps/tcl/tcl8.5/tzdata/America/Denver can't be opened.\n",
      "INFO: [HLS 200-10] For user 'vitis-ai-user' on host 'eceutil1.gpu.snuhpc' (Linux_x86_64 version 4.15.0-76-generic) on Thu Dec 02 02:23:40 MST 2021\n",
      "INFO: [HLS 200-10] On os Ubuntu 18.04.5 LTS\n",
      "INFO: [HLS 200-10] In directory '/workspace/home/hientt/Lab_8/cifar10-hls-test-1'\n",
      "Sourcing Tcl script 'build_prj.tcl'\n",
      "INFO: [HLS 200-10] Opening project '/workspace/home/hientt/Lab_8/cifar10-hls-test-1/myproject_cifar10_cnn4_prj'.\n",
      "INFO: [HLS 200-10] Adding design file 'firmware/myproject_cifar10_cnn4_axi.cpp' to the project\n",
      "INFO: [HLS 200-10] Adding design file 'firmware/myproject_cifar10_cnn4.cpp' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'myproject_cifar10_cnn4_test.cpp' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'firmware/weights' to the project\n",
      "INFO: [HLS 200-10] Adding test bench file 'tb_data' to the project\n",
      "INFO: [HLS 200-10] Opening solution '/workspace/home/hientt/Lab_8/cifar10-hls-test-1/myproject_cifar10_cnn4_prj/solution1'.\n",
      "INFO: [SYN 201-201] Setting up clock 'default' with a period of 5ns.\n",
      "INFO: [HLS 200-10] Setting target device to 'xczu7ev-ffvc1156-2-e'\n",
      "INFO: [XFORM 203-101] Allowed max sub elements number after partition is 4096.\n",
      "INFO: [XFORM 203-1161] The maximum of name length is set into 60.\n",
      "INFO: [XFORM 203-101] Allowed max sub elements number after partition is 4096.\n",
      "INFO: [XFORM 203-1161] The maximum of name length is set into 60.\n",
      "***** C/RTL SYNTHESIS *****\n",
      "INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.\n",
      "INFO: [HLS 200-10] Analyzing design file 'firmware/myproject_cifar10_cnn4.cpp' ... \n",
      "WARNING: [HLS 214-114] Since the only kind of statements allowed in a canonical dataflow region are variable declarations and function calls, the compiler may not be able to correctly handle the region: firmware/nnet_utils/nnet_dense_latency.h:64:9\n",
      "WARNING: [HLS 214-114] Since the only kind of statements allowed in a canonical dataflow region are variable declarations and function calls, the compiler may not be able to correctly handle the region: firmware/nnet_utils/nnet_dense_latency.h:79:2\n",
      "WARNING: [HLS 214-104] Only for-loops and functions support the dataflow: firmware/nnet_utils/nnet_dense_latency.h:76:9\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject_cifar10_cnn4.cpp:64:68\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject_cifar10_cnn4.cpp:64:72\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject_cifar10_cnn4.cpp:76:72\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject_cifar10_cnn4.cpp:76:76\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject_cifar10_cnn4.cpp:92:72\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject_cifar10_cnn4.cpp:92:76\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject_cifar10_cnn4.cpp:104:77\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject_cifar10_cnn4.cpp:104:82\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject_cifar10_cnn4.cpp:124:72\n",
      "WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject_cifar10_cnn4.cpp:124:77\n",
      "WARNING: [HLS 214-114] Since the only kind of statements allowed in a canonical dataflow region are variable declarations and function calls, the compiler may not be able to correctly handle the region: firmware/myproject_cifar10_cnn4.cpp:35:2\n",
      "WARNING: [HLS 214-114] Since the only kind of statements allowed in a canonical dataflow region are variable declarations and function calls, the compiler may not be able to correctly handle the region: firmware/myproject_cifar10_cnn4.cpp:36:5\n",
      "WARNING: [HLS 200-471] Dataflow form checks found 15 issue(s) in file firmware/myproject_cifar10_cnn4.cpp\n",
      "INFO: [HLS 200-10] Analyzing design file 'firmware/myproject_cifar10_cnn4_axi.cpp' ... \n",
      "WARNING: [HLS 214-114] Since the only kind of statements allowed in a canonical dataflow region are variable declarations and function calls, the compiler may not be able to correctly handle the region: firmware/myproject_cifar10_cnn4_axi.cpp:23:2\n",
      "WARNING: [HLS 214-114] Since the only kind of statements allowed in a canonical dataflow region are variable declarations and function calls, the compiler may not be able to correctly handle the region: firmware/myproject_cifar10_cnn4_axi.cpp:35:5\n",
      "WARNING: [HLS 200-471] Dataflow form checks found 2 issue(s) in file firmware/myproject_cifar10_cnn4_axi.cpp\n",
      "INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:00:52 ; elapsed = 00:02:58 . Memory (MB): peak = 954.555 ; gain = 523.035 ; free physical = 24004 ; free virtual = 180061\n",
      "INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:00:52 ; elapsed = 00:02:59 . Memory (MB): peak = 954.555 ; gain = 523.035 ; free physical = 24001 ; free virtual = 180058\n",
      "INFO: [HLS 200-10] Starting code transformations ...\n",
      "INFO: [HLS 200-489] Unrolling loop 'LineBufferShift' (firmware/nnet_utils/nnet_conv_stream.h:228) in function 'void nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config15>(FORWARD_REFERENCE const&, ap_shift_reg<FORWARD_REFERENCE::value_type, FORWARD_REFERENCE::in_width> (*) [FORWARD_REFERENCE::n_chan], FORWARD_REFERENCE::value_type*)' completely with a factor of 1.\n",
      "INFO: [HLS 200-489] Unrolling loop 'LineBufferShift' (firmware/nnet_utils/nnet_conv_stream.h:228) in function 'void nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config8>(FORWARD_REFERENCE const&, ap_shift_reg<FORWARD_REFERENCE::value_type, FORWARD_REFERENCE::in_width> (*) [FORWARD_REFERENCE::n_chan], FORWARD_REFERENCE::value_type*)' completely with a factor of 1.\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>::operator[].1' into 'myproject_cifar10_cnn4_axi' (firmware/myproject_cifar10_cnn4_axi.cpp:27).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>::operator[]' into 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, config2>' (firmware/nnet_utils/nnet_conv_stream.h:223).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::kernel_shift_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, config2>' into 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, config2>' (firmware/nnet_utils/nnet_conv_stream.h:234).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<6, 1, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::limit' into 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2_mult>' (firmware/nnet_utils/nnet_dense_latency.h:57).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<6, 1, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::limit' into 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9_mult>' (firmware/nnet_utils/nnet_dense_latency.h:57).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<6, 1, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::limit' into 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>' (firmware/nnet_utils/nnet_dense_latency.h:57).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<6, 1, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::product' into 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2_mult>' (firmware/nnet_utils/nnet_dense_latency.h:96).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<6, 1, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::product' into 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9_mult>' (firmware/nnet_utils/nnet_dense_latency.h:96).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<6, 1, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::product' into 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>' (firmware/nnet_utils/nnet_dense_latency.h:96).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>::operator[].1' into 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config2>' (firmware/nnet_utils/nnet_conv_stream.h:284).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>::operator[].1' into 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' (firmware/nnet_utils/nnet_conv_stream.h:284).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>::operator[].1' into 'nnet::compute_pool_buffer_2d<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config8>' (firmware/nnet_utils/nnet_pooling_stream.h:195).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>::operator[].1' into 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, linear_config6>' (firmware/nnet_utils/nnet_activation_stream.h:47).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>::operator[].1' into 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, linear_config3>' (firmware/nnet_utils/nnet_activation_stream.h:47).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2_mult>' into 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config2>' (firmware/nnet_utils/nnet_conv_stream.h:276).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config2>' into 'nnet::conv_2d_buffer_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config2>' (firmware/nnet_utils/nnet_conv2d_stream.h:85).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_buffer_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config2>' into 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config2>' (firmware/nnet_utils/nnet_conv2d_stream.h:103).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>::operator[]' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, relu_config7>' (firmware/nnet_utils/nnet_activation_stream.h:69).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>::operator[]' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, relu_config7>' (firmware/nnet_utils/nnet_activation_stream.h:69).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>::operator[]' into 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, linear_config6>' (firmware/nnet_utils/nnet_activation_stream.h:47).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>::operator[]' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, relu_config4>' (firmware/nnet_utils/nnet_activation_stream.h:69).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>::operator[]' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, relu_config4>' (firmware/nnet_utils/nnet_activation_stream.h:69).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>::operator[]' into 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, linear_config3>' (firmware/nnet_utils/nnet_activation_stream.h:47).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>::operator[].1' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, relu_config7>' (firmware/nnet_utils/nnet_activation_stream.h:70).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>::operator[].1' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, relu_config7>' (firmware/nnet_utils/nnet_activation_stream.h:69).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>::operator[].1' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, relu_config4>' (firmware/nnet_utils/nnet_activation_stream.h:70).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>::operator[].1' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, relu_config4>' (firmware/nnet_utils/nnet_activation_stream.h:69).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>::operator[]' into 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config5>' (firmware/nnet_utils/nnet_conv_stream.h:223).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>::operator[]' into 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config8>' (firmware/nnet_utils/nnet_conv_stream.h:223).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::kernel_shift_2d<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config5>' into 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config5>' (firmware/nnet_utils/nnet_conv_stream.h:234).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<6, 1, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::limit' into 'nnet::dense_latency<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5_mult>' (firmware/nnet_utils/nnet_dense_latency.h:57).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<6, 1, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::limit' into 'nnet::dense_latency<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config12_mult>' (firmware/nnet_utils/nnet_dense_latency.h:57).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<6, 1, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::product' into 'nnet::dense_latency<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5_mult>' (firmware/nnet_utils/nnet_dense_latency.h:96).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<6, 1, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0> >::product' into 'nnet::dense_latency<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config12_mult>' (firmware/nnet_utils/nnet_dense_latency.h:96).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::dense_latency<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5_mult>' into 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' (firmware/nnet_utils/nnet_conv_stream.h:276).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' into 'nnet::conv_2d_buffer_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' (firmware/nnet_utils/nnet_conv2d_stream.h:85).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_buffer_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' into 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' (firmware/nnet_utils/nnet_conv2d_stream.h:103).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::kernel_shift_2d<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config8>' into 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config8>' (firmware/nnet_utils/nnet_conv_stream.h:234).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::reduce_pool<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, config8>' into 'nnet::compute_pool_buffer_2d<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config8>' (firmware/nnet_utils/nnet_pooling_stream.h:195).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::compute_pool_buffer_2d<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config8>' into 'nnet::pooling2d_buffer_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config8>' (firmware/nnet_utils/nnet_pooling_stream.h:238).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::pooling2d_buffer_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config8>' into 'nnet::pooling2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config8>' (firmware/nnet_utils/nnet_pooling_stream.h:251).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>::operator[]' into 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config9>' (firmware/nnet_utils/nnet_conv_stream.h:223).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::kernel_shift_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config9>' into 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config9>' (firmware/nnet_utils/nnet_conv_stream.h:234).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>::operator[]' into 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config9>' (firmware/nnet_utils/nnet_conv_stream.h:284).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>::operator[]' into 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config12>' (firmware/nnet_utils/nnet_conv_stream.h:284).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>::operator[]' into 'nnet::compute_pool_buffer_2d<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config15>' (firmware/nnet_utils/nnet_pooling_stream.h:195).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>::operator[]' into 'nnet::repack_stream<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 200u>, 200>' (firmware/nnet_utils/nnet_stream.h:88).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>::operator[]' into 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, linear_config13>' (firmware/nnet_utils/nnet_activation_stream.h:47).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>::operator[]' into 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, linear_config10>' (firmware/nnet_utils/nnet_activation_stream.h:47).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9_mult>' into 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config9>' (firmware/nnet_utils/nnet_conv_stream.h:276).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config9>' into 'nnet::conv_2d_buffer_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config9>' (firmware/nnet_utils/nnet_conv2d_stream.h:85).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_buffer_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config9>' into 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config9>' (firmware/nnet_utils/nnet_conv2d_stream.h:103).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>::operator[]' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, relu_config14>' (firmware/nnet_utils/nnet_activation_stream.h:69).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>::operator[]' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, relu_config14>' (firmware/nnet_utils/nnet_activation_stream.h:69).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>::operator[]' into 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, linear_config13>' (firmware/nnet_utils/nnet_activation_stream.h:47).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>::operator[]' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, relu_config11>' (firmware/nnet_utils/nnet_activation_stream.h:69).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>::operator[]' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, relu_config11>' (firmware/nnet_utils/nnet_activation_stream.h:69).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>::operator[]' into 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, linear_config10>' (firmware/nnet_utils/nnet_activation_stream.h:47).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>::operator[].1' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, relu_config14>' (firmware/nnet_utils/nnet_activation_stream.h:70).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>::operator[].1' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, relu_config14>' (firmware/nnet_utils/nnet_activation_stream.h:69).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>::operator[].1' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, relu_config11>' (firmware/nnet_utils/nnet_activation_stream.h:70).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>::operator[].1' into 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, relu_config11>' (firmware/nnet_utils/nnet_activation_stream.h:69).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>::operator[]' into 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config12>' (firmware/nnet_utils/nnet_conv_stream.h:223).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>::operator[]' into 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config15>' (firmware/nnet_utils/nnet_conv_stream.h:223).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::kernel_shift_2d<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config12>' into 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config12>' (firmware/nnet_utils/nnet_conv_stream.h:234).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::dense_latency<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config12_mult>' into 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config12>' (firmware/nnet_utils/nnet_conv_stream.h:276).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::compute_output_buffer_2d<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config12>' into 'nnet::conv_2d_buffer_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config12>' (firmware/nnet_utils/nnet_conv2d_stream.h:85).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_buffer_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config12>' into 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config12>' (firmware/nnet_utils/nnet_conv2d_stream.h:103).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::kernel_shift_2d<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config15>' into 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config15>' (firmware/nnet_utils/nnet_conv_stream.h:234).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::reduce_pool<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, config15>' into 'nnet::compute_pool_buffer_2d<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config15>' (firmware/nnet_utils/nnet_pooling_stream.h:195).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::compute_pool_buffer_2d<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config15>' into 'nnet::pooling2d_buffer_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config15>' (firmware/nnet_utils/nnet_pooling_stream.h:238).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::pooling2d_buffer_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config15>' into 'nnet::pooling2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config15>' (firmware/nnet_utils/nnet_pooling_stream.h:251).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 200u>::operator[]' into 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 200u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config17>' (firmware/nnet_utils/nnet_dense_stream.h:48).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 200u>::operator[]' into 'nnet::repack_stream<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 200u>, 200>' (firmware/nnet_utils/nnet_stream.h:88).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::dense_latency<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>' into 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>' (firmware/nnet_utils/nnet_dense_stream.h:22).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>::operator[]' into 'myproject_cifar10_cnn4_axi' (firmware/myproject_cifar10_cnn4_axi.cpp:39).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>::operator[]' into 'nnet::softmax_latency<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config19>' (firmware/nnet_utils/nnet_activation_stream.h:175).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>::operator[]' into 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 10u>, linear_config18>' (firmware/nnet_utils/nnet_activation_stream.h:47).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>::operator[]' into 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 200u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config17>' (firmware/nnet_utils/nnet_dense_stream.h:62).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 10u>::operator[]' into 'nnet::softmax_latency<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config19>' (firmware/nnet_utils/nnet_activation_stream.h:156).\n",
      "INFO: [XFORM 203-603] Inlining function 'nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 10u>::operator[]' into 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 10u>, linear_config18>' (firmware/nnet_utils/nnet_activation_stream.h:47).\n",
      "INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 00:01:01 ; elapsed = 00:03:20 . Memory (MB): peak = 961.082 ; gain = 529.562 ; free physical = 23469 ; free virtual = 179526\n",
      "INFO: [HLS 200-10] Checking synthesizability ...\n",
      "INFO: [XFORM 203-602] Inlining function 'fp_struct<double>::mantissa' into 'generic_cast_IEEE754<int, (ap_q_mode)6, double>' (/wrk/2019.2/continuous/2019_11_06_2708876/src/products/hls/hls_lib/hlsmath/include/FloatingPoint/hls_case_IEEE754.h:15) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'fp_struct<double>::expv' into 'generic_cast_IEEE754<int, (ap_q_mode)6, double>' (/wrk/2019.2/continuous/2019_11_06_2708876/src/products/hls/hls_lib/hlsmath/include/FloatingPoint/hls_case_IEEE754.h:18) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'fp_struct<double>::__signbit' into 'generic_cast_IEEE754<int, (ap_q_mode)6, double>' (/wrk/2019.2/continuous/2019_11_06_2708876/src/products/hls/hls_lib/hlsmath/include/FloatingPoint/hls_case_IEEE754.h:59) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'generic_cast_IEEE754<int, (ap_q_mode)6, double>' into 'generic_cast_IEEE754<int, double>' (/wrk/2019.2/continuous/2019_11_06_2708876/src/products/hls/hls_lib/hlsmath/include/FloatingPoint/hls_case_IEEE754.h:117) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'generic_cast_IEEE754<int, double>' into '__hls_fptosi_double_i32' (/wrk/2019.2/continuous/2019_11_06_2708876/src/products/hls/hls_lib/hlsmath/src/lib_floatconversion.cpp:55) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function '__hls_fptosi_double_i32' into 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config2>' (firmware/nnet_utils/nnet_dense_latency.h:56->firmware/nnet_utils/nnet_conv_stream.h:276->firmware/nnet_utils/nnet_conv2d_stream.h:85->firmware/nnet_utils/nnet_conv2d_stream.h:103) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::cast<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2_mult>' into 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config2>' (firmware/nnet_utils/nnet_dense_latency.h:125->firmware/nnet_utils/nnet_conv_stream.h:276->firmware/nnet_utils/nnet_conv2d_stream.h:85->firmware/nnet_utils/nnet_conv2d_stream.h:103) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function '__hls_fptosi_double_i32' into 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' (firmware/nnet_utils/nnet_dense_latency.h:56->firmware/nnet_utils/nnet_conv_stream.h:276->firmware/nnet_utils/nnet_conv2d_stream.h:85->firmware/nnet_utils/nnet_conv2d_stream.h:103) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::cast<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5_mult>' into 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' (firmware/nnet_utils/nnet_dense_latency.h:125->firmware/nnet_utils/nnet_conv_stream.h:276->firmware/nnet_utils/nnet_conv2d_stream.h:85->firmware/nnet_utils/nnet_conv2d_stream.h:103) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0> >::operator()' into 'nnet::reduce<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_max<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:53) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_max<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_max<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:55) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0> >::operator()' into 'nnet::reduce<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_max<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:55) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_max<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::pooling2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config8>' (firmware/nnet_utils/nnet_pooling_stream.h:22->firmware/nnet_utils/nnet_pooling_stream.h:195->firmware/nnet_utils/nnet_pooling_stream.h:238->firmware/nnet_utils/nnet_pooling_stream.h:251) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function '__hls_fptosi_double_i32' into 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config9>' (firmware/nnet_utils/nnet_dense_latency.h:56->firmware/nnet_utils/nnet_conv_stream.h:276->firmware/nnet_utils/nnet_conv2d_stream.h:85->firmware/nnet_utils/nnet_conv2d_stream.h:103) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::cast<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9_mult>' into 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config9>' (firmware/nnet_utils/nnet_dense_latency.h:125->firmware/nnet_utils/nnet_conv_stream.h:276->firmware/nnet_utils/nnet_conv2d_stream.h:85->firmware/nnet_utils/nnet_conv2d_stream.h:103) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function '__hls_fptosi_double_i32' into 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config12>' (firmware/nnet_utils/nnet_dense_latency.h:56->firmware/nnet_utils/nnet_conv_stream.h:276->firmware/nnet_utils/nnet_conv2d_stream.h:85->firmware/nnet_utils/nnet_conv2d_stream.h:103) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::cast<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config12_mult>' into 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config12>' (firmware/nnet_utils/nnet_dense_latency.h:125->firmware/nnet_utils/nnet_conv_stream.h:276->firmware/nnet_utils/nnet_conv2d_stream.h:85->firmware/nnet_utils/nnet_conv2d_stream.h:103) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_max<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::pooling2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config15>' (firmware/nnet_utils/nnet_pooling_stream.h:22->firmware/nnet_utils/nnet_pooling_stream.h:195->firmware/nnet_utils/nnet_pooling_stream.h:238->firmware/nnet_utils/nnet_pooling_stream.h:251) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function '__hls_fptosi_double_i32' into 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>' (firmware/nnet_utils/nnet_dense_latency.h:56->firmware/nnet_utils/nnet_dense_stream.h:22) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::cast<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>' into 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>' (firmware/nnet_utils/nnet_dense_latency.h:125->firmware/nnet_utils/nnet_dense_stream.h:22) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:55) automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 8, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:55) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 8, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:55) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:55) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::softmax_idx_from_real_val<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, softmax_config19>' into 'nnet::softmax_latency<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config19>' (firmware/nnet_utils/nnet_activation_stream.h:156) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::softmax_latency<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config19>' (firmware/nnet_utils/nnet_activation_stream.h:164) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::softmax_idx_from_real_val<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, softmax_config19>' into 'nnet::softmax_latency<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config19>' (firmware/nnet_utils/nnet_activation_stream.h:166) automatically.\n",
      "WARNING: [SYNCHK 200-23] firmware/myproject_cifar10_cnn4_axi.cpp:39: variable-indexed range selection may cause suboptimal QoR.\n",
      "INFO: [SYNCHK 200-10] 0 error(s), 1 warning(s).\n",
      "INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 00:01:05 ; elapsed = 00:03:25 . Memory (MB): peak = 1087.375 ; gain = 655.855 ; free physical = 23244 ; free virtual = 179302\n",
      "WARNING: [XFORM 203-803] Cannot specify interface mode 'axis' on argument 'res.V.data.V' . This interface directive will be discarded. Please apply it on an argument of top module.\n",
      "WARNING: [XFORM 203-803] Cannot specify interface mode 'axis' on argument 'res.V.data.V' . This interface directive will be discarded. Please apply it on an argument of top module.\n",
      "WARNING: [XFORM 203-803] Cannot specify interface mode 'axis' on argument 'data.V.data.V' . This interface directive will be discarded. Please apply it on an argument of top module.\n",
      "WARNING: [XFORM 203-803] Cannot specify interface mode 'axis' (firmware/myproject_cifar10_cnn4.cpp:32:1) on argument 'input_1.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:25). This interface directive will be discarded. Please apply it on an argument of top module.\n",
      "WARNING: [XFORM 203-803] Cannot specify interface mode 'axis' (firmware/myproject_cifar10_cnn4.cpp:32:94) on argument 'layer19_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:26). This interface directive will be discarded. Please apply it on an argument of top module.\n",
      "WARNING: [XFORM 203-1103] Ignored data pack directive on non-struct variable 'out.data' (firmware/myproject_cifar10_cnn4_axi.cpp:5).\n",
      "WARNING: [XFORM 203-1103] Ignored data pack directive on non-struct variable 'in.data' (firmware/myproject_cifar10_cnn4_axi.cpp:4).\n",
      "INFO: [XFORM 203-1101] Packing variable 'out_pack.data.V' (firmware/nnet_utils/nnet_activation_stream.h:170) into a 160-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'out_data.data.V' (firmware/nnet_utils/nnet_activation_stream.h:64) into a 48-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'out_data.data.V' (firmware/nnet_utils/nnet_activation_stream.h:64) into a 48-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'out_data.data.V' (firmware/nnet_utils/nnet_activation_stream.h:64) into a 96-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'out_data.data.V' (firmware/nnet_utils/nnet_activation_stream.h:64) into a 96-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'res_pack.data.V' (firmware/nnet_utils/nnet_pooling_stream.h:178) into a 128-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'res_pack.data.V' (firmware/nnet_utils/nnet_pooling_stream.h:178) into a 256-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'out_data.data.V' (firmware/nnet_utils/nnet_activation_stream.h:42) into a 128-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'out_data.data.V' (firmware/nnet_utils/nnet_activation_stream.h:42) into a 128-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'out_data.data.V' (firmware/nnet_utils/nnet_activation_stream.h:42) into a 256-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'out_data.data.V' (firmware/nnet_utils/nnet_activation_stream.h:42) into a 256-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'out_data.data.V' (firmware/nnet_utils/nnet_activation_stream.h:42) into a 160-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'res_pack.data.V' (firmware/nnet_utils/nnet_dense_stream.h:58) into a 160-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'res_pack.data.V' (firmware/nnet_utils/nnet_conv_stream.h:264) into a 128-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'res_pack.data.V' (firmware/nnet_utils/nnet_conv_stream.h:264) into a 256-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'res_pack.data.V' (firmware/nnet_utils/nnet_conv_stream.h:264) into a 256-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'res_pack.data.V' (firmware/nnet_utils/nnet_conv_stream.h:264) into a 128-bit variable.\n",
      "INFO: [XFORM 203-1101] Packing variable 'ctype.data.V' (firmware/myproject_cifar10_cnn4_axi.cpp:24) into a 48-bit variable.\n",
      "WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (firmware/nnet_utils/nnet_conv_stream.h:188) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config15>'.\n",
      "WARNING: [XFORM 203-505] Ignore pipeline pragma in Loop whose tripcount is only 1 (firmware/nnet_utils/nnet_conv_stream.h:188) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config8>'.\n",
      "INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::softmax_latency<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config19>' (firmware/nnet_utils/nnet_activation_stream.h:146:59).\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'Loop-1' (firmware/nnet_utils/nnet_stream.h:82) in function 'nnet::repack_stream<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 200u>, 200>' for pipelining.\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ReLUActLoop' (firmware/nnet_utils/nnet_activation_stream.h:60) in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, relu_config14>' for pipelining.\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ReLUActLoop' (firmware/nnet_utils/nnet_activation_stream.h:60) in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, relu_config11>' for pipelining.\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ReLUActLoop' (firmware/nnet_utils/nnet_activation_stream.h:60) in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, relu_config7>' for pipelining.\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ReLUActLoop' (firmware/nnet_utils/nnet_activation_stream.h:60) in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, relu_config4>' for pipelining.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ReadInputWidth' (firmware/nnet_utils/nnet_pooling_stream.h:234) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config15>' for pipelining.\n",
      "WARNING: [XFORM 203-505] Ignored pipeline directive for loop 'FiltLoop' (firmware/nnet_utils/nnet_pooling_stream.h:186) because its parent loop or function is pipelined.\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ReadInputWidth' (firmware/nnet_utils/nnet_pooling_stream.h:234) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config8>' for pipelining.\n",
      "WARNING: [XFORM 203-505] Ignored pipeline directive for loop 'FiltLoop' (firmware/nnet_utils/nnet_pooling_stream.h:186) because its parent loop or function is pipelined.\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LinearActLoop' (firmware/nnet_utils/nnet_activation_stream.h:38) in function 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, linear_config13>' for pipelining.\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LinearActLoop' (firmware/nnet_utils/nnet_activation_stream.h:38) in function 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, linear_config10>' for pipelining.\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LinearActLoop' (firmware/nnet_utils/nnet_activation_stream.h:38) in function 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, linear_config6>' for pipelining.\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'LinearActLoop' (firmware/nnet_utils/nnet_activation_stream.h:38) in function 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, linear_config3>' for pipelining.\n",
      "INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 10u>, linear_config18>' (firmware/nnet_utils/nnet_activation_stream.h:38:51).\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ReadInputWidth' (firmware/nnet_utils/nnet_conv2d_stream.h:79) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config12>' for pipelining.\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ReadInputWidth' (firmware/nnet_utils/nnet_conv2d_stream.h:79) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' for pipelining.\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ReadInputWidth' (firmware/nnet_utils/nnet_conv2d_stream.h:79) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config2>' for pipelining.\n",
      "INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'ReadInputWidth' (firmware/nnet_utils/nnet_conv2d_stream.h:79) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config9>' for pipelining.\n",
      "INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>' (firmware/nnet_utils/nnet_dense_latency.h:20:55).\n",
      "INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config15>' (firmware/nnet_utils/nnet_conv_stream.h:187:82).\n",
      "INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config12>' (firmware/nnet_utils/nnet_conv_stream.h:187:82).\n",
      "WARNING: [XFORM 203-505] Ignored pipeline directive for loop 'KernelShiftWidth' (firmware/nnet_utils/nnet_conv_stream.h:187) because its parent loop or function is pipelined.\n",
      "INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config9>' (firmware/nnet_utils/nnet_conv_stream.h:187:82).\n",
      "WARNING: [XFORM 203-505] Ignored pipeline directive for loop 'KernelShiftWidth' (firmware/nnet_utils/nnet_conv_stream.h:187) because its parent loop or function is pipelined.\n",
      "INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config8>' (firmware/nnet_utils/nnet_conv_stream.h:187:82).\n",
      "INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config5>' (firmware/nnet_utils/nnet_conv_stream.h:187:82).\n",
      "WARNING: [XFORM 203-505] Ignored pipeline directive for loop 'KernelShiftWidth' (firmware/nnet_utils/nnet_conv_stream.h:187) because its parent loop or function is pipelined.\n",
      "INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, config2>' (firmware/nnet_utils/nnet_conv_stream.h:187:82).\n",
      "WARNING: [XFORM 203-505] Ignored pipeline directive for loop 'KernelShiftWidth' (firmware/nnet_utils/nnet_conv_stream.h:187) because its parent loop or function is pipelined.\n",
      "INFO: [HLS 200-489] Unrolling loop 'SoftmaxExpPackLoop' (firmware/nnet_utils/nnet_activation_stream.h:154) in function 'nnet::softmax_latency<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config19>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'SoftmaxInvPackLoop' (firmware/nnet_utils/nnet_activation_stream.h:172) in function 'nnet::softmax_latency<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config19>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Loop-1.1' (firmware/nnet_utils/nnet_stream.h:86) in function 'nnet::repack_stream<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 200u>, 200>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Loop-1.2' (firmware/nnet_utils/nnet_stream.h:92) in function 'nnet::repack_stream<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 200u>, 200>' completely with a factor of 199.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ReLUPackLoop' (firmware/nnet_utils/nnet_activation_stream.h:67) in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, relu_config14>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ReLUPackLoop' (firmware/nnet_utils/nnet_activation_stream.h:67) in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, relu_config11>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ReLUPackLoop' (firmware/nnet_utils/nnet_activation_stream.h:67) in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, relu_config7>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ReLUPackLoop' (firmware/nnet_utils/nnet_activation_stream.h:67) in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, relu_config4>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'FiltLoop' (firmware/nnet_utils/nnet_pooling_stream.h:186) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config15>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'PoolLoop' (firmware/nnet_utils/nnet_pooling_stream.h:190) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config15>' completely with a factor of 4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [HLS 200-489] Unrolling loop 'FiltLoop' (firmware/nnet_utils/nnet_pooling_stream.h:186) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config8>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'PoolLoop' (firmware/nnet_utils/nnet_pooling_stream.h:190) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config8>' completely with a factor of 4.\n",
      "INFO: [HLS 200-489] Unrolling loop 'LinearPackLoop' (firmware/nnet_utils/nnet_activation_stream.h:45) in function 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, linear_config13>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'LinearPackLoop' (firmware/nnet_utils/nnet_activation_stream.h:45) in function 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, linear_config10>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'LinearPackLoop' (firmware/nnet_utils/nnet_activation_stream.h:45) in function 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, linear_config6>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'LinearPackLoop' (firmware/nnet_utils/nnet_activation_stream.h:45) in function 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, linear_config3>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'LinearPackLoop' (firmware/nnet_utils/nnet_activation_stream.h:45) in function 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 10u>, linear_config18>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'DataPack' (firmware/nnet_utils/nnet_dense_stream.h:46) in function 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 200u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config17>' completely with a factor of 200.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ResPack' (firmware/nnet_utils/nnet_dense_stream.h:60) in function 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 200u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config17>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_dense_latency.h:85) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config12>' completely with a factor of 72.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_dense_latency.h:90) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config12>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_dense_latency.h:101) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config12>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_dense_latency.h:109) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config12>' completely with a factor of 72.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_dense_latency.h:113) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config12>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_latency.h:120) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config12>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'CastLoop' (firmware/nnet_utils/nnet_conv_stream.h:282) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config12>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Loop-1.1.1' (firmware/nnet_utils/nnet_conv_stream.h:258) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 143.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_dense_latency.h:85) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 144.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_dense_latency.h:90) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_dense_latency.h:101) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_dense_latency.h:109) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 144.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_dense_latency.h:113) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_latency.h:120) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'CastLoop' (firmware/nnet_utils/nnet_conv_stream.h:282) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_dense_latency.h:85) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config2>' completely with a factor of 27.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_dense_latency.h:90) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config2>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_dense_latency.h:101) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config2>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_dense_latency.h:109) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config2>' completely with a factor of 27.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_dense_latency.h:113) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config2>' completely with a factor of 16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_latency.h:120) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config2>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'CastLoop' (firmware/nnet_utils/nnet_conv_stream.h:282) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config2>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Loop-1.1.1' (firmware/nnet_utils/nnet_conv_stream.h:258) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config9>' completely with a factor of 143.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_dense_latency.h:85) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config9>' completely with a factor of 144.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_dense_latency.h:90) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config9>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_dense_latency.h:101) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config9>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_dense_latency.h:109) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config9>' completely with a factor of 144.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_dense_latency.h:113) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config9>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_latency.h:120) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config9>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'CastLoop' (firmware/nnet_utils/nnet_conv_stream.h:282) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config9>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_dense_latency.h:85) in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>' completely with a factor of 200.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_dense_latency.h:90) in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_dense_latency.h:101) in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_dense_latency.h:109) in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>' completely with a factor of 200.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_dense_latency.h:113) in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_dense_latency.h:120) in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>' completely with a factor of 10.\n",
      "INFO: [HLS 200-489] Unrolling loop 'UpdateBuffer' (firmware/nnet_utils/nnet_conv_stream.h:219) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config15>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'LineBufferDataIn' (firmware/nnet_utils/nnet_conv_stream.h:226) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config15>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftWidth' (firmware/nnet_utils/nnet_conv_stream.h:187) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config15>' completely with a factor of 1.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftHeight' (firmware/nnet_utils/nnet_conv_stream.h:189) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config15>' completely with a factor of 2.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftChannel' (firmware/nnet_utils/nnet_conv_stream.h:190) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config15>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelPushHeight' (firmware/nnet_utils/nnet_conv_stream.h:199) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config15>' completely with a factor of 2.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelPushChannel' (firmware/nnet_utils/nnet_conv_stream.h:201) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config15>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'UpdateBuffer' (firmware/nnet_utils/nnet_conv_stream.h:219) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config12>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'LineBufferDataIn' (firmware/nnet_utils/nnet_conv_stream.h:226) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config12>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'LineBufferShift' (firmware/nnet_utils/nnet_conv_stream.h:228) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config12>' completely with a factor of 2.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftWidth' (firmware/nnet_utils/nnet_conv_stream.h:187) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config12>' completely with a factor of 2.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftHeight' (firmware/nnet_utils/nnet_conv_stream.h:189) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config12>' completely with a factor of 3.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftChannel' (firmware/nnet_utils/nnet_conv_stream.h:190) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config12>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelPushHeight' (firmware/nnet_utils/nnet_conv_stream.h:199) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config12>' completely with a factor of 3.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelPushChannel' (firmware/nnet_utils/nnet_conv_stream.h:201) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config12>' completely with a factor of 8.\n",
      "INFO: [HLS 200-489] Unrolling loop 'UpdateBuffer' (firmware/nnet_utils/nnet_conv_stream.h:219) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config9>' completely with a factor of 16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [HLS 200-489] Unrolling loop 'LineBufferDataIn' (firmware/nnet_utils/nnet_conv_stream.h:226) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config9>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'LineBufferShift' (firmware/nnet_utils/nnet_conv_stream.h:228) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config9>' completely with a factor of 2.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftWidth' (firmware/nnet_utils/nnet_conv_stream.h:187) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config9>' completely with a factor of 2.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftHeight' (firmware/nnet_utils/nnet_conv_stream.h:189) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config9>' completely with a factor of 3.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftChannel' (firmware/nnet_utils/nnet_conv_stream.h:190) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config9>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelPushHeight' (firmware/nnet_utils/nnet_conv_stream.h:199) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config9>' completely with a factor of 3.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelPushChannel' (firmware/nnet_utils/nnet_conv_stream.h:201) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config9>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'UpdateBuffer' (firmware/nnet_utils/nnet_conv_stream.h:219) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config8>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'LineBufferDataIn' (firmware/nnet_utils/nnet_conv_stream.h:226) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config8>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftWidth' (firmware/nnet_utils/nnet_conv_stream.h:187) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config8>' completely with a factor of 1.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftHeight' (firmware/nnet_utils/nnet_conv_stream.h:189) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config8>' completely with a factor of 2.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftChannel' (firmware/nnet_utils/nnet_conv_stream.h:190) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config8>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelPushHeight' (firmware/nnet_utils/nnet_conv_stream.h:199) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config8>' completely with a factor of 2.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelPushChannel' (firmware/nnet_utils/nnet_conv_stream.h:201) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config8>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'UpdateBuffer' (firmware/nnet_utils/nnet_conv_stream.h:219) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config5>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'LineBufferDataIn' (firmware/nnet_utils/nnet_conv_stream.h:226) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config5>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'LineBufferShift' (firmware/nnet_utils/nnet_conv_stream.h:228) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config5>' completely with a factor of 2.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftWidth' (firmware/nnet_utils/nnet_conv_stream.h:187) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config5>' completely with a factor of 2.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftHeight' (firmware/nnet_utils/nnet_conv_stream.h:189) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config5>' completely with a factor of 3.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftChannel' (firmware/nnet_utils/nnet_conv_stream.h:190) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config5>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelPushHeight' (firmware/nnet_utils/nnet_conv_stream.h:199) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config5>' completely with a factor of 3.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelPushChannel' (firmware/nnet_utils/nnet_conv_stream.h:201) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config5>' completely with a factor of 16.\n",
      "INFO: [HLS 200-489] Unrolling loop 'UpdateBuffer' (firmware/nnet_utils/nnet_conv_stream.h:219) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, config2>' completely with a factor of 3.\n",
      "INFO: [HLS 200-489] Unrolling loop 'LineBufferDataIn' (firmware/nnet_utils/nnet_conv_stream.h:226) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, config2>' completely with a factor of 3.\n",
      "INFO: [HLS 200-489] Unrolling loop 'LineBufferShift' (firmware/nnet_utils/nnet_conv_stream.h:228) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, config2>' completely with a factor of 2.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftWidth' (firmware/nnet_utils/nnet_conv_stream.h:187) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, config2>' completely with a factor of 2.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftHeight' (firmware/nnet_utils/nnet_conv_stream.h:189) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, config2>' completely with a factor of 3.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelShiftChannel' (firmware/nnet_utils/nnet_conv_stream.h:190) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, config2>' completely with a factor of 3.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelPushHeight' (firmware/nnet_utils/nnet_conv_stream.h:199) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, config2>' completely with a factor of 3.\n",
      "INFO: [HLS 200-489] Unrolling loop 'KernelPushChannel' (firmware/nnet_utils/nnet_conv_stream.h:201) in function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, config2>' completely with a factor of 3.\n",
      "INFO: [XFORM 203-102] Automatically partitioning shift register array 'line_buffer.Array.V.5' .\n",
      "INFO: [XFORM 203-102] Automatically partitioning shift register array 'line_buffer.Array.V.4' .\n",
      "INFO: [XFORM 203-102] Automatically partitioning shift register array 'line_buffer.Array.V.3' .\n",
      "INFO: [XFORM 203-102] Automatically partitioning shift register array 'line_buffer.Array.V.2' .\n",
      "INFO: [XFORM 203-102] Automatically partitioning shift register array 'line_buffer.Array.V.1' .\n",
      "INFO: [XFORM 203-102] Automatically partitioning shift register array 'line_buffer.Array.V' .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer18_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:126) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'out_local.V.data.V' (firmware/myproject_cifar10_cnn4_axi.cpp:18) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer15_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:114) .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer20_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:118) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer13_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:106) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer14_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:110) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer10_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:94) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer11_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:98) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer6_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:78) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer7_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:82) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer3_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:66) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer4_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:70) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer8_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:86) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer12_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:102) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer9_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:90) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer5_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:74) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer2_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:62) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'layer17_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:122) .\n",
      "INFO: [XFORM 203-102] Automatically partitioning streamed array 'in_local.V.data.V' (firmware/myproject_cifar10_cnn4_axi.cpp:17) .\n",
      "INFO: [XFORM 203-101] Partitioning array 'exp_res.V' (firmware/nnet_utils/nnet_activation_stream.h:146) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'tmp.data.V' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'tmp.data.V.2' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'tmp.data.V' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'tmp.data.V' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'tmp.data.V' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'tmp.data.V' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'line_buffer.Array.V.5' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'pool_window.V' (firmware/nnet_utils/nnet_pooling_stream.h:172) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'kernel_data.V.5'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'line_buffer.Array.V.4' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'pool_window.V' (firmware/nnet_utils/nnet_pooling_stream.h:172) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'kernel_data.V.4'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'tmp.data.V' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'tmp.data.V' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'tmp.data.V' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'tmp.data.V' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'tmp.data.V' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'data.V' (firmware/nnet_utils/nnet_dense_stream.h:35) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'res'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'tmp.data.V' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'line_buffer.Array.V.3' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'kernel_data.V.3'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'res_out.i.i'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'b12.V'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'mult.V' (firmware/nnet_utils/nnet_dense_latency.h:39) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'acc.V' (firmware/nnet_utils/nnet_dense_latency.h:40) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'line_buffer.Array.V.2' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'kernel_data.V.2'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'res_out.i.i'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'b5.V'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'mult.V' (firmware/nnet_utils/nnet_dense_latency.h:39) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'acc.V' (firmware/nnet_utils/nnet_dense_latency.h:40) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'line_buffer.Array.V.1' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'kernel_data.V.1'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'res_out.i.i'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'b2.V'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'mult.V' (firmware/nnet_utils/nnet_dense_latency.h:39) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'acc.V' (firmware/nnet_utils/nnet_dense_latency.h:40) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'line_buffer.Array.V' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'kernel_data.V'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'res_out.i.i'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'b9.V'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'mult.V' (firmware/nnet_utils/nnet_dense_latency.h:39) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'acc.V' (firmware/nnet_utils/nnet_dense_latency.h:40) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'tmp.data.V' in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'b17.V'  in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'mult.V' (firmware/nnet_utils/nnet_dense_latency.h:39) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'acc.V' (firmware/nnet_utils/nnet_dense_latency.h:40) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'shift_buffer.V' (firmware/nnet_utils/nnet_conv_stream.h:216) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'shift_buffer.V' (firmware/nnet_utils/nnet_conv_stream.h:216) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'shift_buffer.V' (firmware/nnet_utils/nnet_conv_stream.h:216) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'shift_buffer.V' (firmware/nnet_utils/nnet_conv_stream.h:216) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'shift_buffer.V' (firmware/nnet_utils/nnet_conv_stream.h:216) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'shift_buffer.V' (firmware/nnet_utils/nnet_conv_stream.h:216) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer18_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:126) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'out_local.V.data.V' (firmware/myproject_cifar10_cnn4_axi.cpp:18) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer15_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:114) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer20_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:118) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer13_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:106) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer14_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:110) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer10_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:94) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer11_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:98) in dimension 1 completely.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [XFORM 203-101] Partitioning array 'layer6_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:78) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer7_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:82) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer3_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:66) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer4_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:70) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer8_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:86) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer12_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:102) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer9_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:90) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer5_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:74) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer2_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:62) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'layer17_out.V.data.V' (firmware/myproject_cifar10_cnn4.cpp:122) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'in_local.V.data.V' (firmware/myproject_cifar10_cnn4_axi.cpp:17) in dimension 1 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'line_buffer.Array.V.5'  in dimension 2 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'line_buffer.Array.V.4'  in dimension 2 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'line_buffer.Array.V.3'  in dimension 2 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'line_buffer.Array.V.2'  in dimension 2 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'line_buffer.Array.V.1'  in dimension 2 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'line_buffer.Array.V'  in dimension 2 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'shift_buffer.V' (firmware/nnet_utils/nnet_conv_stream.h:216) in dimension 2 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'shift_buffer.V' (firmware/nnet_utils/nnet_conv_stream.h:216) in dimension 2 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'shift_buffer.V' (firmware/nnet_utils/nnet_conv_stream.h:216) in dimension 2 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'shift_buffer.V' (firmware/nnet_utils/nnet_conv_stream.h:216) in dimension 2 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'shift_buffer.V' (firmware/nnet_utils/nnet_conv_stream.h:216) in dimension 2 completely.\n",
      "INFO: [XFORM 203-101] Partitioning array 'shift_buffer.V' (firmware/nnet_utils/nnet_conv_stream.h:216) in dimension 2 completely.\n",
      "INFO: [XFORM 203-602] Inlining function 'fp_struct<double>::mantissa' into 'generic_cast_IEEE754<int, (ap_q_mode)6, double>' (/wrk/2019.2/continuous/2019_11_06_2708876/src/products/hls/hls_lib/hlsmath/include/FloatingPoint/hls_case_IEEE754.h:15) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'fp_struct<double>::expv' into 'generic_cast_IEEE754<int, (ap_q_mode)6, double>' (/wrk/2019.2/continuous/2019_11_06_2708876/src/products/hls/hls_lib/hlsmath/include/FloatingPoint/hls_case_IEEE754.h:18) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'fp_struct<double>::__signbit' into 'generic_cast_IEEE754<int, (ap_q_mode)6, double>' (/wrk/2019.2/continuous/2019_11_06_2708876/src/products/hls/hls_lib/hlsmath/include/FloatingPoint/hls_case_IEEE754.h:59) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'generic_cast_IEEE754<int, (ap_q_mode)6, double>' into 'generic_cast_IEEE754<int, double>' (/wrk/2019.2/continuous/2019_11_06_2708876/src/products/hls/hls_lib/hlsmath/include/FloatingPoint/hls_case_IEEE754.h:117) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'generic_cast_IEEE754<int, double>' into '__hls_fptosi_double_i32' (/wrk/2019.2/continuous/2019_11_06_2708876/src/products/hls/hls_lib/hlsmath/src/lib_floatconversion.cpp:55) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function '__hls_fptosi_double_i32' into 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config2>' (firmware/nnet_utils/nnet_dense_latency.h:56->firmware/nnet_utils/nnet_conv_stream.h:276->firmware/nnet_utils/nnet_conv2d_stream.h:85->firmware/nnet_utils/nnet_conv2d_stream.h:103) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::cast<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config2_mult>' into 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config2>' (firmware/nnet_utils/nnet_dense_latency.h:125->firmware/nnet_utils/nnet_conv_stream.h:276->firmware/nnet_utils/nnet_conv2d_stream.h:85->firmware/nnet_utils/nnet_conv2d_stream.h:103) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function '__hls_fptosi_double_i32' into 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' (firmware/nnet_utils/nnet_dense_latency.h:56->firmware/nnet_utils/nnet_conv_stream.h:276->firmware/nnet_utils/nnet_conv2d_stream.h:85->firmware/nnet_utils/nnet_conv2d_stream.h:103) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::cast<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config5_mult>' into 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' (firmware/nnet_utils/nnet_dense_latency.h:125->firmware/nnet_utils/nnet_conv_stream.h:276->firmware/nnet_utils/nnet_conv2d_stream.h:85->firmware/nnet_utils/nnet_conv2d_stream.h:103) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0> >::operator()' into 'nnet::reduce<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_max<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:53) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_max<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_max<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:55) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_max<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0> >::operator()' into 'nnet::reduce<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_max<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:55) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function '__hls_fptosi_double_i32' into 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config9>' (firmware/nnet_utils/nnet_dense_latency.h:56->firmware/nnet_utils/nnet_conv_stream.h:276->firmware/nnet_utils/nnet_conv2d_stream.h:85->firmware/nnet_utils/nnet_conv2d_stream.h:103) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::cast<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config9_mult>' into 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config9>' (firmware/nnet_utils/nnet_dense_latency.h:125->firmware/nnet_utils/nnet_conv_stream.h:276->firmware/nnet_utils/nnet_conv2d_stream.h:85->firmware/nnet_utils/nnet_conv2d_stream.h:103) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function '__hls_fptosi_double_i32' into 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config12>' (firmware/nnet_utils/nnet_dense_latency.h:56->firmware/nnet_utils/nnet_conv_stream.h:276->firmware/nnet_utils/nnet_conv2d_stream.h:85->firmware/nnet_utils/nnet_conv2d_stream.h:103) automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [XFORM 203-602] Inlining function 'nnet::cast<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config12_mult>' into 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config12>' (firmware/nnet_utils/nnet_dense_latency.h:125->firmware/nnet_utils/nnet_conv_stream.h:276->firmware/nnet_utils/nnet_conv2d_stream.h:85->firmware/nnet_utils/nnet_conv2d_stream.h:103) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_max<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::pooling2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config15>' (firmware/nnet_utils/nnet_pooling_stream.h:22->firmware/nnet_utils/nnet_pooling_stream.h:195->firmware/nnet_utils/nnet_pooling_stream.h:238->firmware/nnet_utils/nnet_pooling_stream.h:251) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function '__hls_fptosi_double_i32' into 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>' (firmware/nnet_utils/nnet_dense_latency.h:56->firmware/nnet_utils/nnet_dense_stream.h:22) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::cast<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>' into 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>' (firmware/nnet_utils/nnet_dense_latency.h:125->firmware/nnet_utils/nnet_dense_stream.h:22) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >::operator()' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:53) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >::operator()' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:55) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >::operator()' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 8, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:55) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 8, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:55) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:55) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> >::operator()' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:55) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::softmax_idx_from_real_val<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, softmax_config19>' into 'nnet::softmax_latency<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config19>' (firmware/nnet_utils/nnet_activation_stream.h:156) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::softmax_idx_from_real_val<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, softmax_config19>' into 'nnet::softmax_latency<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config19>' (firmware/nnet_utils/nnet_activation_stream.h:166) automatically.\n",
      "INFO: [XFORM 203-721] Changing loop 'Loop_1_proc' (firmware/myproject_cifar10_cnn4_axi.cpp:23) to a process function for dataflow in function 'myproject_cifar10_cnn4_axi'.\n",
      "INFO: [XFORM 203-721] Changing loop 'Loop_2_proc' (firmware/myproject_cifar10_cnn4_axi.cpp:37) to a process function for dataflow in function 'myproject_cifar10_cnn4_axi'.\n",
      "INFO: [XFORM 203-712] Applying dataflow to function 'myproject_cifar10_cnn4', detected/extracted 19 process function(s): \n",
      "\t 'myproject_cifar10_cnn4_Block__proc'\n",
      "\t 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config2>'\n",
      "\t 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, linear_config3>'\n",
      "\t 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, relu_config4>'\n",
      "\t 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>'\n",
      "\t 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, linear_config6>'\n",
      "\t 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, relu_config7>'\n",
      "\t 'nnet::pooling2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config8>'\n",
      "\t 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config9>'\n",
      "\t 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, linear_config10>'\n",
      "\t 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, relu_config11>'\n",
      "\t 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config12>'\n",
      "\t 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, linear_config13>'\n",
      "\t 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, relu_config14>'\n",
      "\t 'nnet::pooling2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config15>'\n",
      "\t 'nnet::repack_stream<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 200u>, 200>'\n",
      "\t 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 200u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config17>'\n",
      "\t 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 10u>, linear_config18>'\n",
      "\t 'nnet::softmax<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config19>'.\n",
      "INFO: [XFORM 203-712] Applying dataflow to function 'myproject_cifar10_cnn4_axi', detected/extracted 5 process function(s): \n",
      "\t 'Block_codeRepl51_proc'\n",
      "\t 'Loop_1_proc541'\n",
      "\t 'myproject_cifar10_cnn4'\n",
      "\t 'Block_myproject_cifar10_cnn4_axi_.exit52_proc'\n",
      "\t 'Loop_2_proc'.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (firmware/nnet_utils/nnet_activation_stream.h:60:74) to (firmware/nnet_utils/nnet_activation_stream.h:60:68) in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, relu_config14>'... converting 25 basic blocks.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (firmware/nnet_utils/nnet_activation_stream.h:60:74) to (firmware/nnet_utils/nnet_activation_stream.h:60:68) in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, relu_config11>'... converting 25 basic blocks.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (firmware/nnet_utils/nnet_activation_stream.h:60:74) to (firmware/nnet_utils/nnet_activation_stream.h:60:68) in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, relu_config7>'... converting 49 basic blocks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (firmware/nnet_utils/nnet_activation_stream.h:60:74) to (firmware/nnet_utils/nnet_activation_stream.h:60:68) in function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, relu_config4>'... converting 49 basic blocks.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (firmware/nnet_utils/nnet_common.h:55:43) in function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >'... converting 5 basic blocks.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock to (firmware/nnet_utils/nnet_common.h:53:17) in function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >'... converting 5 basic blocks.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (firmware/nnet_utils/nnet_common.h:45:20) to (firmware/nnet_utils/nnet_common.h:55:43) in function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >'... converting 13 basic blocks.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (firmware/myproject_cifar10_cnn4_axi.cpp:38:90) to (firmware/myproject_cifar10_cnn4_axi.cpp:37:49) in function 'Loop_2_proc'... converting 10 basic blocks.\n",
      "INFO: [XFORM 203-401] Performing if-conversion on hyperblock from (firmware/myproject_cifar10_cnn4_axi.cpp:28:25) to (firmware/myproject_cifar10_cnn4_axi.cpp:26:41) in function 'Loop_1_proc541'... converting 10 basic blocks.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_max<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::pooling2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config8>' (firmware/nnet_utils/nnet_pooling_stream.h:22->firmware/nnet_utils/nnet_pooling_stream.h:195->firmware/nnet_utils/nnet_pooling_stream.h:238->firmware/nnet_utils/nnet_pooling_stream.h:251) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 2, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:55) automatically.\n",
      "INFO: [XFORM 203-602] Inlining function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 4, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' into 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' (firmware/nnet_utils/nnet_common.h:55->firmware/nnet_utils/nnet_common.h:55) automatically.\n",
      "INFO: [XFORM 203-11] Balancing expressions in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config15>' (firmware/nnet_utils/nnet_pooling_stream.h:86:5)...3 expression(s) balanced.\n",
      "INFO: [XFORM 203-11] Balancing expressions in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config8>' (firmware/nnet_utils/nnet_pooling_stream.h:86:5)...3 expression(s) balanced.\n",
      "INFO: [XFORM 203-11] Balancing expressions in function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>' (firmware/nnet_utils/nnet_mult.h:20:9)...1868 expression(s) balanced.\n",
      "INFO: [XFORM 203-11] Balancing expressions in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config12>' (firmware/nnet_utils/nnet_conv_stream.h:45:5)...463 expression(s) balanced.\n",
      "INFO: [XFORM 203-11] Balancing expressions in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' (firmware/nnet_utils/nnet_conv_stream.h:45:5)...1690 expression(s) balanced.\n",
      "INFO: [XFORM 203-11] Balancing expressions in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config2>' (firmware/nnet_utils/nnet_conv_stream.h:45:5)...297 expression(s) balanced.\n",
      "INFO: [XFORM 203-11] Balancing expressions in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config9>' (firmware/nnet_utils/nnet_conv_stream.h:45:5)...932 expression(s) balanced.\n",
      "INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:09:16 ; elapsed = 00:12:08 . Memory (MB): peak = 6586.586 ; gain = 6155.066 ; free physical = 18685 ; free virtual = 174883\n",
      "INFO: [XFORM 203-541] Flattening a loop nest 'ReadInputHeight' (firmware/nnet_utils/nnet_pooling_stream.h:233:80) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config15>'.\n",
      "INFO: [XFORM 203-541] Flattening a loop nest 'ReadInputHeight' (firmware/nnet_utils/nnet_pooling_stream.h:233:80) in function 'nnet::pooling2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config8>'.\n",
      "INFO: [XFORM 203-541] Flattening a loop nest 'ReadInputHeight' (firmware/nnet_utils/nnet_conv2d_stream.h:78:80) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config12>'.\n",
      "INFO: [XFORM 203-541] Flattening a loop nest 'ReadInputHeight' (firmware/nnet_utils/nnet_conv2d_stream.h:78:80) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>'.\n",
      "INFO: [XFORM 203-541] Flattening a loop nest 'ReadInputHeight' (firmware/nnet_utils/nnet_conv2d_stream.h:78:80) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config2>'.\n",
      "INFO: [XFORM 203-541] Flattening a loop nest 'ReadInputHeight' (firmware/nnet_utils/nnet_conv2d_stream.h:78:80) in function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config9>'.\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::softmax_latency<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config19>' to 'softmax_latency<array,array,softmax_config19>' (firmware/nnet_utils/nnet_activation_stream.h:149:1)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::softmax<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, softmax_config19>' to 'softmax<array,array<ap_fixed,10u>,softmax_config19>' (firmware/nnet_utils/nnet_activation_stream.h:333:1)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config15>' to 'shift_line_buffer<array<ap_fixed,8u>,config15>' (firmware/nnet_utils/nnet_conv_stream.h:213:1)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, config12>' to 'shift_line_buffer<array<ap_fixed,8u>,config12>' (firmware/nnet_utils/nnet_conv_stream.h:213:1)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config8>' to 'shift_line_buffer<array<ap_fixed,16u>,config8>' (firmware/nnet_utils/nnet_conv_stream.h:213:1)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::shift_line_buffer<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, config5>' to 'shift_line_buffer<array<ap_fixed,16u>,config5>' (firmware/nnet_utils/nnet_conv_stream.h:213:1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: [XFORM 203-631] Renaming function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, config2>' to 'shift_line_buffer<array<ap_fixed,3u>,config2>' (firmware/nnet_utils/nnet_conv_stream.h:213:1)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::shift_line_buffer<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config9>' to 'shift_line_buffer<array<ap_fixed,16u>,config9>' (firmware/nnet_utils/nnet_conv_stream.h:213:1)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::repack_stream<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 200u>, 200>' to 'repack_stream<array,array<ap_fixed,200u>,200>' (firmware/nnet_utils/nnet_stream.h:79:47)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, relu_config14>' to 'relu<array,array<ap_fixed,8u>,relu_config14>' (firmware/nnet_utils/nnet_activation_stream.h:60:68)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, relu_config11>' to 'relu<array,array<ap_fixed,8u>,relu_config11>' (firmware/nnet_utils/nnet_activation_stream.h:60:68)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, relu_config7>' to 'relu<array,array<ap_fixed,16u>,relu_config7>' (firmware/nnet_utils/nnet_activation_stream.h:60:68)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::relu<nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, relu_config4>' to 'relu<array,array<ap_fixed,16u>,relu_config4>' (firmware/nnet_utils/nnet_activation_stream.h:60:68)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::reduce<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0>, 10, nnet::Op_add<ap_fixed<18, 8, (ap_q_mode)0, (ap_o_mode)0, 0> > >' to 'reduce<ap_fixed,10,Op_add<ap_fixed<18,8,0,0,0>>>' (firmware/nnet_utils/nnet_common.h:45:43)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::pooling2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config15>' to 'pooling2d_cl<array,array<ap_fixed,8u>,config15>' (firmware/nnet_utils/nnet_pooling_stream.h:86:5)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::pooling2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config8>' to 'pooling2d_cl<array,array<ap_fixed,16u>,config8>' (firmware/nnet_utils/nnet_pooling_stream.h:86:5)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, linear_config13>' to 'linear<array,array<ap_fixed,8u>,linear_config13>' (firmware/nnet_utils/nnet_activation_stream.h:38:70)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, linear_config10>' to 'linear<array,array<ap_fixed,8u>,linear_config10>' (firmware/nnet_utils/nnet_activation_stream.h:38:70)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, linear_config6>' to 'linear<array,array<ap_fixed,16u>,linear_config6>' (firmware/nnet_utils/nnet_activation_stream.h:38:70)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, linear_config3>' to 'linear<array,array<ap_fixed,16u>,linear_config3>' (firmware/nnet_utils/nnet_activation_stream.h:38:70)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::linear<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)0, (ap_o_mode)0, 0>, 10u>, linear_config18>' to 'linear<array,array<ap_fixed,10u>,linear_config18>' (firmware/nnet_utils/nnet_activation_stream.h:38:1)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::dense_wrapper<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, config17>' to 'dense_wrapper<ap_fixed,ap_fixed<16,6,5,3,0>,config17>' (firmware/nnet_utils/nnet_mult.h:20:9)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::dense<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 200u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 10u>, config17>' to 'dense<array,array<ap_fixed<16,6,5,3,0>,10u>,config17>' (firmware/nnet_utils/nnet_dense_stream.h:41:1)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 8u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config12>' to 'conv_2d_cl<array,array<ap_fixed,8u>,config12>' (firmware/nnet_utils/nnet_conv_stream.h:45:5)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::conv_2d_cl<nnet::array<ap_fixed<6, 1, (ap_q_mode)0, (ap_o_mode)0, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config5>' to 'conv_2d_cl<array,array<ap_fixed,16u>,config5>' (firmware/nnet_utils/nnet_conv_stream.h:45:5)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 3u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, config2>' to 'conv_2d_cl<array,array<ap_fixed,16u>,config2>' (firmware/nnet_utils/nnet_conv_stream.h:45:5)\n",
      "WARNING: [XFORM 203-631] Renaming function 'nnet::conv_2d_cl<nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 16u>, nnet::array<ap_fixed<16, 6, (ap_q_mode)5, (ap_o_mode)3, 0>, 8u>, config9>' to 'conv_2d_cl<array,array<ap_fixed,8u>,config9>' (firmware/nnet_utils/nnet_conv_stream.h:45:5)\n",
      "INFO: [HLS 200-472] Inferring partial write operation for 'out_data.data.V' (firmware/nnet_utils/nnet_stream.h:88:42)\n",
      "INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:13:00 ; elapsed = 00:15:56 . Memory (MB): peak = 6586.586 ; gain = 6155.066 ; free physical = 11907 ; free virtual = 168179\n",
      "INFO: [HLS 200-10] Starting hardware synthesis ...\n",
      "INFO: [HLS 200-10] Synthesizing 'myproject_cifar10_cnn4_axi' ...\n",
      "WARNING: [SYN 201-103] Legalizing function name 'shift_line_buffer<array<ap_fixed,3u>,config2>' to 'shift_line_buffer_array_ap_fixed_3u_config2_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'conv_2d_cl<array,array<ap_fixed,16u>,config2>' to 'conv_2d_cl_array_array_ap_fixed_16u_config2_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'linear<array,array<ap_fixed,16u>,linear_config3>' to 'linear_array_array_ap_fixed_16u_linear_config3_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'relu<array,array<ap_fixed,16u>,relu_config4>' to 'relu_array_array_ap_fixed_16u_relu_config4_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'shift_line_buffer<array<ap_fixed,16u>,config5>' to 'shift_line_buffer_array_ap_fixed_16u_config5_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'conv_2d_cl<array,array<ap_fixed,16u>,config5>' to 'conv_2d_cl_array_array_ap_fixed_16u_config5_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'linear<array,array<ap_fixed,16u>,linear_config6>' to 'linear_array_array_ap_fixed_16u_linear_config6_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'relu<array,array<ap_fixed,16u>,relu_config7>' to 'relu_array_array_ap_fixed_16u_relu_config7_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'shift_line_buffer<array<ap_fixed,16u>,config8>' to 'shift_line_buffer_array_ap_fixed_16u_config8_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'pooling2d_cl<array,array<ap_fixed,16u>,config8>' to 'pooling2d_cl_array_array_ap_fixed_16u_config8_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'shift_line_buffer<array<ap_fixed,16u>,config9>' to 'shift_line_buffer_array_ap_fixed_16u_config9_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'conv_2d_cl<array,array<ap_fixed,8u>,config9>' to 'conv_2d_cl_array_array_ap_fixed_8u_config9_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'linear<array,array<ap_fixed,8u>,linear_config10>' to 'linear_array_array_ap_fixed_8u_linear_config10_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'relu<array,array<ap_fixed,8u>,relu_config11>' to 'relu_array_array_ap_fixed_8u_relu_config11_s'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: [SYN 201-103] Legalizing function name 'shift_line_buffer<array<ap_fixed,8u>,config12>' to 'shift_line_buffer_array_ap_fixed_8u_config12_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'conv_2d_cl<array,array<ap_fixed,8u>,config12>' to 'conv_2d_cl_array_array_ap_fixed_8u_config12_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'linear<array,array<ap_fixed,8u>,linear_config13>' to 'linear_array_array_ap_fixed_8u_linear_config13_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'relu<array,array<ap_fixed,8u>,relu_config14>' to 'relu_array_array_ap_fixed_8u_relu_config14_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'shift_line_buffer<array<ap_fixed,8u>,config15>' to 'shift_line_buffer_array_ap_fixed_8u_config15_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'pooling2d_cl<array,array<ap_fixed,8u>,config15>' to 'pooling2d_cl_array_array_ap_fixed_8u_config15_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'repack_stream<array,array<ap_fixed,200u>,200>' to 'repack_stream_array_array_ap_fixed_200u_200_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense_wrapper<ap_fixed,ap_fixed<16,6,5,3,0>,config17>' to 'dense_wrapper_ap_fixed_ap_fixed_16_6_5_3_0_config17_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'dense<array,array<ap_fixed<16,6,5,3,0>,10u>,config17>' to 'dense_array_array_ap_fixed_16_6_5_3_0_10u_config17_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'linear<array,array<ap_fixed,10u>,linear_config18>' to 'linear_array_array_ap_fixed_10u_linear_config18_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'reduce<ap_fixed,10,Op_add<ap_fixed<18,8,0,0,0>>>' to 'reduce_ap_fixed_10_Op_add_ap_fixed_18_8_0_0_0_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'softmax_latency<array,array,softmax_config19>' to 'softmax_latency_array_array_softmax_config19_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'softmax<array,array<ap_fixed,10u>,softmax_config19>' to 'softmax_array_array_ap_fixed_10u_softmax_config19_s'.\n",
      "WARNING: [SYN 201-103] Legalizing function name 'Block_myproject_cifar10_cnn4_axi_.exit52_proc' to 'Block_myproject_cifar10_cnn4_axi_exit52_proc'.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'Loop_1_proc541' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 959.33 seconds; current allocated memory: 894.375 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.99 seconds; current allocated memory: 894.813 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'shift_line_buffer_array_ap_fixed_3u_config2_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'shift_line_buffer<array<ap_fixed,3u>,config2>'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 1.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.3 seconds; current allocated memory: 895.092 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.91 seconds; current allocated memory: 895.333 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'conv_2d_cl_array_array_ap_fixed_16u_config2_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReadInputHeight_ReadInputWidth'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 4.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 2.18 seconds; current allocated memory: 900.279 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Starting global binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 8.42 seconds; current allocated memory: 905.824 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'linear_array_array_ap_fixed_16u_linear_config3_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'LinearActLoop'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 8.53 seconds; current allocated memory: 906.410 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.29 seconds; current allocated memory: 906.793 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'relu_array_array_ap_fixed_16u_relu_config4_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReLUActLoop'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.89 seconds; current allocated memory: 908.024 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 2.87 seconds; current allocated memory: 909.811 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'shift_line_buffer_array_ap_fixed_16u_config5_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'shift_line_buffer<array<ap_fixed,16u>,config5>'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 1.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 3.79 seconds; current allocated memory: 910.892 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 2.1 seconds; current allocated memory: 911.985 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'conv_2d_cl_array_array_ap_fixed_16u_config5_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReadInputHeight_ReadInputWidth'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 6.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 11.76 seconds; current allocated memory: 934.550 MB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Starting global binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 38.82 seconds; current allocated memory: 961.851 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'linear_array_array_ap_fixed_16u_linear_config6_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'LinearActLoop'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 31.02 seconds; current allocated memory: 963.495 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.85 seconds; current allocated memory: 963.879 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'relu_array_array_ap_fixed_16u_relu_config7_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReLUActLoop'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.09 seconds; current allocated memory: 965.175 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 2.53 seconds; current allocated memory: 966.962 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'shift_line_buffer_array_ap_fixed_16u_config8_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'shift_line_buffer<array<ap_fixed,16u>,config8>'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 1.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 3.18 seconds; current allocated memory: 967.603 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.17 seconds; current allocated memory: 968.115 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'pooling2d_cl_array_array_ap_fixed_16u_config8_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReadInputHeight_ReadInputWidth'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.76 seconds; current allocated memory: 969.321 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 2.49 seconds; current allocated memory: 970.722 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'shift_line_buffer_array_ap_fixed_16u_config9_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'shift_line_buffer<array<ap_fixed,16u>,config9>'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 1.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 2.73 seconds; current allocated memory: 971.741 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.83 seconds; current allocated memory: 972.833 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'conv_2d_cl_array_array_ap_fixed_8u_config9_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReadInputHeight_ReadInputWidth'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 8.02 seconds; current allocated memory: 990.155 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Starting global binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 23.67 seconds; current allocated memory: 1010.053 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'linear_array_array_ap_fixed_8u_linear_config10_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'LinearActLoop'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 24.09 seconds; current allocated memory: 1011.465 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.9 seconds; current allocated memory: 1011.704 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'relu_array_array_ap_fixed_8u_relu_config11_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReLUActLoop'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.34 seconds; current allocated memory: 1012.336 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.66 seconds; current allocated memory: 1013.276 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'shift_line_buffer_array_ap_fixed_8u_config12_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'shift_line_buffer<array<ap_fixed,8u>,config12>'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 1.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 2.29 seconds; current allocated memory: 1013.820 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.38 seconds; current allocated memory: 1014.389 MB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'conv_2d_cl_array_array_ap_fixed_8u_config12_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReadInputHeight_ReadInputWidth'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 5.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 3.56 seconds; current allocated memory: 1022.287 MB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Starting global binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 11.4 seconds; current allocated memory: 1.007 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'linear_array_array_ap_fixed_8u_linear_config13_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'LinearActLoop'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 11.59 seconds; current allocated memory: 1.007 GB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.75 seconds; current allocated memory: 1.007 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'relu_array_array_ap_fixed_8u_relu_config14_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReLUActLoop'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.23 seconds; current allocated memory: 1.008 GB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.65 seconds; current allocated memory: 1.009 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'shift_line_buffer_array_ap_fixed_8u_config15_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'shift_line_buffer<array<ap_fixed,8u>,config15>'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 1.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 2.05 seconds; current allocated memory: 1.009 GB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.96 seconds; current allocated memory: 1.010 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'pooling2d_cl_array_array_ap_fixed_8u_config15_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'ReadInputHeight_ReadInputWidth'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 3.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.31 seconds; current allocated memory: 1.010 GB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.5 seconds; current allocated memory: 1.011 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'repack_stream_array_array_ap_fixed_200u_200_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining loop 'Loop 1'.\n",
      "WARNING: [SCHED 204-69] Unable to schedule 'store' operation ('out_data_data_V_addr_2_write_ln88', firmware/nnet_utils/nnet_stream.h:88) of variable 'tmp_data_V_2_2', firmware/nnet_utils/nnet_stream.h:85 on array 'din.data.V', firmware/nnet_utils/nnet_stream.h:79 due to limited memory ports. Please consider using a memory core with more ports or partitioning the array 'out_data_data_V'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 104, Depth = 106.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 5.9 seconds; current allocated memory: 1.013 GB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 4.84 seconds; current allocated memory: 1.018 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_wrapper_ap_fixed_ap_fixed_16_6_5_3_0_config17_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'dense_wrapper<ap_fixed,ap_fixed<16,6,5,3,0>,config17>'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 9.8 seconds; current allocated memory: 1.043 GB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Starting global binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 53.01 seconds; current allocated memory: 1.080 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'dense_array_array_ap_fixed_16_6_5_3_0_10u_config17_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 42.83 seconds; current allocated memory: 1.083 GB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 3.8 seconds; current allocated memory: 1.086 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'linear_array_array_ap_fixed_10u_linear_config18_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'linear<array,array<ap_fixed,10u>,linear_config18>'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 1.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 6.94 seconds; current allocated memory: 1.088 GB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.96 seconds; current allocated memory: 1.089 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'reduce_ap_fixed_10_Op_add_ap_fixed_18_8_0_0_0_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'reduce<ap_fixed,10,Op_add<ap_fixed<18,8,0,0,0>>>'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 2.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.26 seconds; current allocated memory: 1.089 GB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.35 seconds; current allocated memory: 1.089 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'softmax_latency_array_array_softmax_config19_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-61] Pipelining function 'softmax_latency<array,array,softmax_config19>'.\n",
      "INFO: [SCHED 204-61] Pipelining result : Target II = 1, Final II = 1, Depth = 6.\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.87 seconds; current allocated memory: 1.090 GB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Starting global binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.85 seconds; current allocated memory: 1.090 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'softmax_array_array_ap_fixed_10u_softmax_config19_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.86 seconds; current allocated memory: 1.090 GB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.06 seconds; current allocated memory: 1.091 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'myproject_cifar10_cnn4' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.71 seconds; current allocated memory: 1.093 GB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 29.95 seconds; current allocated memory: 1.115 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'Block_myproject_cifar10_cnn4_axi_exit52_proc' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 16.62 seconds; current allocated memory: 1.121 GB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 0.91 seconds; current allocated memory: 1.121 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'Loop_2_proc' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1 seconds; current allocated memory: 1.121 GB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.05 seconds; current allocated memory: 1.121 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-42] -- Implementing module 'myproject_cifar10_cnn4_axi' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SCHED 204-11] Starting scheduling ...\n",
      "INFO: [SCHED 204-11] Finished scheduling.\n",
      "INFO: [HLS 200-111]  Elapsed time: 1.38 seconds; current allocated memory: 1.122 GB.\n",
      "INFO: [BIND 205-100] Starting micro-architecture generation ...\n",
      "INFO: [BIND 205-101] Performing variable lifetime analysis.\n",
      "INFO: [BIND 205-101] Exploring resource sharing.\n",
      "INFO: [BIND 205-101] Binding ...\n",
      "INFO: [BIND 205-100] Finished micro-architecture generation.\n",
      "INFO: [HLS 200-111]  Elapsed time: 11.84 seconds; current allocated memory: 1.129 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'Loop_1_proc541' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_cifar10_cnn4_axi_fpext_32ns_64_2_1': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'Loop_1_proc541'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 12.53 seconds; current allocated memory: 1.136 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'shift_line_buffer_array_ap_fixed_3u_config2_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_3u_config2_s_line_buffer_Array_V_1_0_0' to 'shift_line_buffer_array_ap_fixed_3u_config2_s_line_bufferbkb' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_3u_config2_s_line_buffer_Array_V_1_1_0' to 'shift_line_buffer_array_ap_fixed_3u_config2_s_line_buffercud' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_3u_config2_s_line_buffer_Array_V_1_0_1' to 'shift_line_buffer_array_ap_fixed_3u_config2_s_line_bufferdEe' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_3u_config2_s_line_buffer_Array_V_1_1_1' to 'shift_line_buffer_array_ap_fixed_3u_config2_s_line_buffereOg' due to the length limit 60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_3u_config2_s_line_buffer_Array_V_1_0_2' to 'shift_line_buffer_array_ap_fixed_3u_config2_s_line_bufferfYi' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_3u_config2_s_line_buffer_Array_V_1_1_2' to 'shift_line_buffer_array_ap_fixed_3u_config2_s_line_bufferg8j' due to the length limit 60\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'shift_line_buffer_array_ap_fixed_3u_config2_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 9.32 seconds; current allocated memory: 1.140 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'conv_2d_cl_array_array_ap_fixed_16u_config2_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_3' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_4' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_5' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_6' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_7' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_8' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_12' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_13' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_14' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_15' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_16' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_17' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_21' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_22' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_23' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_24' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_25' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_1_26' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sX_4' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sY_4' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'pY_4' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'pX_4' is power-on initialization.\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'conv_2d_cl_array_array_ap_fixed_16u_config2_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 7.49 seconds; current allocated memory: 1.149 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'linear_array_array_ap_fixed_16u_linear_config3_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_array_array_ap_fixed_16u_linear_config3_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 95.49 seconds; current allocated memory: 1.172 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'relu_array_array_ap_fixed_16u_relu_config4_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'relu_array_array_ap_fixed_16u_relu_config4_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 4.2 seconds; current allocated memory: 1.176 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'shift_line_buffer_array_ap_fixed_16u_config5_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_0_0' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffehbi' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_1_0' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffeibs' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_0_1' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffejbC' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_1_1' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffekbM' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_0_2' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffelbW' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_1_2' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffemb6' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_0_3' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffencg' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_1_3' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffeocq' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_0_4' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffepcA' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_1_4' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffeqcK' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_0_5' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffercU' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_1_5' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffesc4' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_0_6' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffetde' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_1_6' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffeudo' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_0_7' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffevdy' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_1_7' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffewdI' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_0_8' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffexdS' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_1_8' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffeyd2' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_0_9' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffezec' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_1_9' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffeAem' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_0_10' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffeBew' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_1_10' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffeCeG' due to the length limit 60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_0_11' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffeDeQ' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_1_11' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffeEe0' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_0_12' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffeFfa' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_1_12' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffeGfk' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_0_13' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffeHfu' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_1_13' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffeIfE' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_0_14' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffeJfO' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_1_14' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffeKfY' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_0_15' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffeLf8' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffer_Array_V_2_1_15' to 'shift_line_buffer_array_ap_fixed_16u_config5_s_line_buffeMgi' due to the length limit 60\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'shift_line_buffer_array_ap_fixed_16u_config5_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 33.83 seconds; current allocated memory: 1.229 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'conv_2d_cl_array_array_ap_fixed_16u_config5_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_16' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_17' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_18' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_19' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_20' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_21' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_22' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_23' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_24' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_25' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_26' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_27' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_28' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_29' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_30' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_31' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_32' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_33' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_34' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_35' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_36' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_37' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_38' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_39' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_40' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_41' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_42' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_43' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_44' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_45' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_46' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_47' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_64' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_65' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_66' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_67' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_68' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_69' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_70' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_71' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_72' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_73' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_74' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_75' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_76' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_77' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_78' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_79' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_80' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_81' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_82' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_83' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_84' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_85' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_86' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_87' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_88' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_89' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_90' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_91' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_92' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_93' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_94' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_95' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_112' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_113' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_114' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_115' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_116' is power-on initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_117' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_118' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_119' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_120' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_121' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_122' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_123' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_124' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_125' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_126' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_127' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_128' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_129' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_130' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_131' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_132' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_133' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_134' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_135' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_136' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_137' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_138' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_139' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_140' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_141' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_142' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_2_143' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sX_3' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sY_3' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'pY_3' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'pX_3' is power-on initialization.\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'conv_2d_cl_array_array_ap_fixed_16u_config5_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 35.82 seconds; current allocated memory: 1.288 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'linear_array_array_ap_fixed_16u_linear_config6_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_array_array_ap_fixed_16u_linear_config6_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 417.21 seconds; current allocated memory: 1.391 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'relu_array_array_ap_fixed_16u_relu_config7_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'relu_array_array_ap_fixed_16u_relu_config7_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 8.21 seconds; current allocated memory: 1.394 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'shift_line_buffer_array_ap_fixed_16u_config8_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffer_Array_V_4_0_0' to 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffeNgs' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffer_Array_V_4_0_1' to 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffeOgC' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffer_Array_V_4_0_2' to 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffePgM' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffer_Array_V_4_0_3' to 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffeQgW' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffer_Array_V_4_0_4' to 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffeRg6' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffer_Array_V_4_0_5' to 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffeShg' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffer_Array_V_4_0_6' to 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffeThq' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffer_Array_V_4_0_7' to 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffeUhA' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffer_Array_V_4_0_8' to 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffeVhK' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffer_Array_V_4_0_9' to 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffeWhU' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffer_Array_V_4_0_10' to 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffeXh4' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffer_Array_V_4_0_11' to 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffeYie' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffer_Array_V_4_0_12' to 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffeZio' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffer_Array_V_4_0_13' to 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffe0iy' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffer_Array_V_4_0_14' to 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffe1iI' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffer_Array_V_4_0_15' to 'shift_line_buffer_array_ap_fixed_16u_config8_s_line_buffe2iS' due to the length limit 60\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'shift_line_buffer_array_ap_fixed_16u_config8_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 28.26 seconds; current allocated memory: 1.410 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'pooling2d_cl_array_array_ap_fixed_16u_config8_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "WARNING: [RTGEN 206-101] Register 'pX_1' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sX_1' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'pY_1' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sY_1' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_16' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_17' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_18' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_19' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_20' is power-on initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_21' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_22' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_23' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_24' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_25' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_26' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_27' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_28' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_29' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_30' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_31' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_48' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_49' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_50' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_51' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_52' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_53' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_54' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_55' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_56' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_57' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_58' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_59' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_60' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_61' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_62' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_4_63' is power-on initialization.\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'pooling2d_cl_array_array_ap_fixed_16u_config8_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 16.93 seconds; current allocated memory: 1.420 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'shift_line_buffer_array_ap_fixed_16u_config9_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_0_0' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffe3i2' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_1450_0' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffe4jc' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_0_1' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffe5jm' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_1450_1' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffe6jw' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_0_2' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffe7jG' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_1450_2' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffe8jQ' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_0_3' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffe9j0' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_1450_3' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffebak' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_0_4' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffebbk' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_1450_4' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffebck' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_0_5' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffebdk' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_1450_5' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffebek' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_0_6' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffebfk' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_1450_6' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffebgk' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_0_7' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffebhl' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_1450_7' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffebil' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_0_8' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffebjl' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_1450_8' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffebkl' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_0_9' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffebll' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_1450_9' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffebml' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_0_10' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffebnm' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_1450_10' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffebom' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_0_11' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffebpm' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_1450_11' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffebqm' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_0_12' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffebrm' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_1450_12' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffebsm' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_0_13' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffebtn' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_1450_13' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffebun' due to the length limit 60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_0_14' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffebvn' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_1450_14' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffebwn' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_0_15' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffebxn' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffer_Array_V_1450_15' to 'shift_line_buffer_array_ap_fixed_16u_config9_s_line_buffebyn' due to the length limit 60\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'shift_line_buffer_array_ap_fixed_16u_config9_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 31.67 seconds; current allocated memory: 1.472 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'conv_2d_cl_array_array_ap_fixed_8u_config9_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_16' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_17' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_18' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_19' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_20' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_21' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_22' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_23' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_24' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_25' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_26' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_27' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_28' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_29' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_30' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_31' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_32' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_33' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_34' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_35' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_36' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_37' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_38' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_39' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_40' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_41' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_42' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_43' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_44' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_45' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_46' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_47' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_64' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_65' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_66' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_67' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_68' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_69' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_70' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_71' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_72' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_73' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_74' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_75' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_76' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_77' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_78' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_79' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_80' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_81' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_82' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_83' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_84' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_85' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_86' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_87' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_88' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_89' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_90' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_91' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_92' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_93' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_94' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_95' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_112' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_113' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_114' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_115' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_116' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_117' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_118' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_119' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_120' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_121' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_122' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_123' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_124' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_125' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_126' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_127' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_128' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_129' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_130' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_131' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_132' is power-on initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_133' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_134' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_135' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_136' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_137' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_138' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_139' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_140' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_141' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_142' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_143' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sX_5' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sY_5' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'pY_5' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'pX_5' is power-on initialization.\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'conv_2d_cl_array_array_ap_fixed_8u_config9_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 36.33 seconds; current allocated memory: 1.527 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'linear_array_array_ap_fixed_8u_linear_config10_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_array_array_ap_fixed_8u_linear_config10_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 262.25 seconds; current allocated memory: 1.604 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'relu_array_array_ap_fixed_8u_relu_config11_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'relu_array_array_ap_fixed_8u_relu_config11_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 14.84 seconds; current allocated memory: 1.606 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'shift_line_buffer_array_ap_fixed_8u_config12_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffer_Array_V_3_0_0' to 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffebzo' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffer_Array_V_3_1_0' to 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffebAo' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffer_Array_V_3_0_1' to 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffebBo' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffer_Array_V_3_1_1' to 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffebCo' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffer_Array_V_3_0_2' to 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffebDo' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffer_Array_V_3_1_2' to 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffebEo' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffer_Array_V_3_0_3' to 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffebFp' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffer_Array_V_3_1_3' to 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffebGp' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffer_Array_V_3_0_4' to 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffebHp' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffer_Array_V_3_1_4' to 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffebIp' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffer_Array_V_3_0_5' to 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffebJp' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffer_Array_V_3_1_5' to 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffebKp' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffer_Array_V_3_0_6' to 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffebLp' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffer_Array_V_3_1_6' to 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffebMq' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffer_Array_V_3_0_7' to 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffebNq' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffer_Array_V_3_1_7' to 'shift_line_buffer_array_ap_fixed_8u_config12_s_line_buffebOq' due to the length limit 60\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'shift_line_buffer_array_ap_fixed_8u_config12_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 28.35 seconds; current allocated memory: 1.622 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'conv_2d_cl_array_array_ap_fixed_8u_config12_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_8' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_9' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_10' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_11' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_12' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_13' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_14' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_15' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_16' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_17' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_18' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_19' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_20' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_21' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_22' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_23' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_32' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_33' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_34' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_35' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_36' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_37' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_38' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_39' is power-on initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_40' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_41' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_42' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_43' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_44' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_45' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_46' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_47' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_56' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_57' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_58' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_59' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_60' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_61' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_62' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_63' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_64' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_65' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_66' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_67' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_68' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_69' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_70' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_3_71' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sX_2' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sY_2' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'pY_2' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'pX_2' is power-on initialization.\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'conv_2d_cl_array_array_ap_fixed_8u_config12_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 27.78 seconds; current allocated memory: 1.641 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'linear_array_array_ap_fixed_8u_linear_config13_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_array_array_ap_fixed_8u_linear_config13_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 144.64 seconds; current allocated memory: 1.677 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'relu_array_array_ap_fixed_8u_relu_config14_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'relu_array_array_ap_fixed_8u_relu_config14_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 15.3 seconds; current allocated memory: 1.679 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'shift_line_buffer_array_ap_fixed_8u_config15_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8u_config15_s_line_buffer_Array_V_5_0_0' to 'shift_line_buffer_array_ap_fixed_8u_config15_s_line_buffebPq' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8u_config15_s_line_buffer_Array_V_5_0_1' to 'shift_line_buffer_array_ap_fixed_8u_config15_s_line_buffebQq' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8u_config15_s_line_buffer_Array_V_5_0_2' to 'shift_line_buffer_array_ap_fixed_8u_config15_s_line_buffebRq' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8u_config15_s_line_buffer_Array_V_5_0_3' to 'shift_line_buffer_array_ap_fixed_8u_config15_s_line_buffebSr' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8u_config15_s_line_buffer_Array_V_5_0_4' to 'shift_line_buffer_array_ap_fixed_8u_config15_s_line_buffebTr' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8u_config15_s_line_buffer_Array_V_5_0_5' to 'shift_line_buffer_array_ap_fixed_8u_config15_s_line_buffebUr' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8u_config15_s_line_buffer_Array_V_5_0_6' to 'shift_line_buffer_array_ap_fixed_8u_config15_s_line_buffebVr' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'shift_line_buffer_array_ap_fixed_8u_config15_s_line_buffer_Array_V_5_0_7' to 'shift_line_buffer_array_ap_fixed_8u_config15_s_line_buffebWr' due to the length limit 60\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'shift_line_buffer_array_ap_fixed_8u_config15_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 25.04 seconds; current allocated memory: 1.685 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'pooling2d_cl_array_array_ap_fixed_8u_config15_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "WARNING: [RTGEN 206-101] Register 'pX' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sX' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'pY' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'sY' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_5_8' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_5_9' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_5_10' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_5_11' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_5_12' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_5_13' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_5_14' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_5_15' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_5_24' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_5_25' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_5_26' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_5_27' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_5_28' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_5_29' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_5_30' is power-on initialization.\n",
      "WARNING: [RTGEN 206-101] Register 'kernel_data_V_5_31' is power-on initialization.\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'pooling2d_cl_array_array_ap_fixed_8u_config15_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 18.91 seconds; current allocated memory: 1.688 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'repack_stream_array_array_ap_fixed_200u_200_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SYN 201-210] Renamed object name 'repack_stream_array_array_ap_fixed_200u_200_s_out_data_data_V' to 'repack_stream_array_array_ap_fixed_200u_200_s_out_data_dabXr' due to the length limit 60\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'repack_stream_array_array_ap_fixed_200u_200_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 23.46 seconds; current allocated memory: 1.698 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_wrapper_ap_fixed_ap_fixed_16_6_5_3_0_config17_s' \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-104] Estimated max fanout for 'dense_wrapper_ap_fixed_ap_fixed_16_6_5_3_0_config17_s' is 24272 from HDL expression: (1'b0 == ap_block_pp0_stage0)\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_wrapper_ap_fixed_ap_fixed_16_6_5_3_0_config17_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 40.63 seconds; current allocated memory: 1.756 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'dense_array_array_ap_fixed_16_6_5_3_0_10u_config17_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'dense_array_array_ap_fixed_16_6_5_3_0_10u_config17_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 428.33 seconds; current allocated memory: 1.935 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'linear_array_array_ap_fixed_10u_linear_config18_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'linear_array_array_ap_fixed_10u_linear_config18_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 33.9 seconds; current allocated memory: 1.941 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'reduce_ap_fixed_10_Op_add_ap_fixed_18_8_0_0_0_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'reduce_ap_fixed_10_Op_add_ap_fixed_18_8_0_0_0_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 20.27 seconds; current allocated memory: 1.942 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'softmax_latency_array_array_softmax_config19_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SYN 201-210] Renamed object name 'softmax_latency_array_array_softmax_config19_s_invert_table2' to 'softmax_latency_array_array_softmax_config19_s_invert_tabbYs' due to the length limit 60\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_latency_array_array_softmax_config19_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 23.16 seconds; current allocated memory: 1.945 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'softmax_array_array_ap_fixed_10u_softmax_config19_s' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'softmax_array_array_ap_fixed_10u_softmax_config19_s'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 21.06 seconds; current allocated memory: 1.947 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'myproject_cifar10_cnn4' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [SYN 201-210] Renamed object name 'start_for_dense_array_array_ap_fixed_16_6_5_3_0_10u_config17_U0' to 'start_for_dense_array_array_ap_fixed_16_6_5_3_0_10u_confibZs' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'start_for_linear_array_array_ap_fixed_10u_linear_config18_U0' to 'start_for_linear_array_array_ap_fixed_10u_linear_config18b0s' due to the length limit 60\n",
      "INFO: [SYN 201-210] Renamed object name 'start_for_softmax_array_array_ap_fixed_10u_softmax_config19_U0' to 'start_for_softmax_array_array_ap_fixed_10u_softmax_configb1s' due to the length limit 60\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'myproject_cifar10_cnn4'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 22.08 seconds; current allocated memory: 1.963 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'Block_myproject_cifar10_cnn4_axi_exit52_proc' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'Block_myproject_cifar10_cnn4_axi_exit52_proc'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 35.08 seconds; current allocated memory: 1.982 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'Loop_2_proc' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-100] Generating core module 'myproject_cifar10_cnn4_axi_mux_104_16_1_1': 1 instance(s).\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'Loop_2_proc'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 13.15 seconds; current allocated memory: 1.983 GB.\n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [HLS 200-10] -- Generating RTL for module 'myproject_cifar10_cnn4_axi' \n",
      "INFO: [HLS 200-10] ----------------------------------------------------------------\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject_cifar10_cnn4_axi/in_data' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject_cifar10_cnn4_axi/in_last_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject_cifar10_cnn4_axi/out_data' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on port 'myproject_cifar10_cnn4_axi/out_last_V' to 'axis' (register, both mode).\n",
      "INFO: [RTGEN 206-500] Setting interface mode on function 'myproject_cifar10_cnn4_axi' to 'ap_ctrl_none'.\n",
      "WARNING: [HLS 200-631] Ignoring ap_ctrl_none interface for myproject_cifar10_cnn4_axi due to Block_myproject_cifar10_cnn4_axi_.exit52_proc with non-FIFO I/O\n",
      "INFO: [RTGEN 206-100] Finished creating RTL model for 'myproject_cifar10_cnn4_axi'.\n",
      "INFO: [HLS 200-111]  Elapsed time: 16.28 seconds; current allocated memory: 1.985 GB.\n",
      "INFO: [HLS 200-790] **** Loop Constraint Status: All loop constraints were satisfied.\n",
      "INFO: [HLS 200-789] **** Estimated Fmax: 228.62 MHz\n",
      "INFO: [RTMG 210-278] Implementing memory 'repack_stream_array_array_ap_fixed_200u_200_s_out_data_dabXr_ram (RAM)' using block RAMs.\n",
      "INFO: [RTMG 210-279] Implementing memory 'softmax_latency_array_array_softmax_config19_s_exp_table1_rom' using block ROMs.\n",
      "INFO: [RTMG 210-279] Implementing memory 'softmax_latency_array_array_softmax_config19_s_invert_tabbYs_rom' using auto ROMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_0_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_1_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_2_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_3_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_4_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_5_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_6_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_7_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_8_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_9_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_10_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_11_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_12_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_13_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_14_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer2_out_V_data_15_V_U(fifo_w16_d900_A)' using Block RAMs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_0_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_1_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_2_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_3_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_4_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_5_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_6_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_7_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_8_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_9_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_10_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_11_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_12_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_13_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_14_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer3_out_V_data_15_V_U(fifo_w16_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_V_data_0_V_U(fifo_w6_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_V_data_1_V_U(fifo_w6_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_V_data_2_V_U(fifo_w6_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_V_data_3_V_U(fifo_w6_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_V_data_4_V_U(fifo_w6_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_V_data_5_V_U(fifo_w6_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_V_data_6_V_U(fifo_w6_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_V_data_7_V_U(fifo_w6_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_V_data_8_V_U(fifo_w6_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_V_data_9_V_U(fifo_w6_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_V_data_10_V_U(fifo_w6_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_V_data_11_V_U(fifo_w6_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_V_data_12_V_U(fifo_w6_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_V_data_13_V_U(fifo_w6_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_V_data_14_V_U(fifo_w6_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer4_out_V_data_15_V_U(fifo_w6_d900_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_0_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_1_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_2_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_3_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_4_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_5_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_6_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_7_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_8_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_9_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_10_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_11_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_12_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_13_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_14_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer5_out_V_data_15_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_0_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_1_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_2_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_3_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_4_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_5_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_6_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_7_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_8_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_9_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_10_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_11_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_12_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_13_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_14_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer6_out_V_data_15_V_U(fifo_w16_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_0_V_U(fifo_w6_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_1_V_U(fifo_w6_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_2_V_U(fifo_w6_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_3_V_U(fifo_w6_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_4_V_U(fifo_w6_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_5_V_U(fifo_w6_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_6_V_U(fifo_w6_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_7_V_U(fifo_w6_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_8_V_U(fifo_w6_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_9_V_U(fifo_w6_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_10_V_U(fifo_w6_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_11_V_U(fifo_w6_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_12_V_U(fifo_w6_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_13_V_U(fifo_w6_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_14_V_U(fifo_w6_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer7_out_V_data_15_V_U(fifo_w6_d784_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer8_out_V_data_0_V_U(fifo_w16_d196_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer8_out_V_data_1_V_U(fifo_w16_d196_A)' using Block RAMs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [RTMG 210-285] Implementing FIFO 'layer8_out_V_data_2_V_U(fifo_w16_d196_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer8_out_V_data_3_V_U(fifo_w16_d196_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer8_out_V_data_4_V_U(fifo_w16_d196_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer8_out_V_data_5_V_U(fifo_w16_d196_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer8_out_V_data_6_V_U(fifo_w16_d196_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer8_out_V_data_7_V_U(fifo_w16_d196_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer8_out_V_data_8_V_U(fifo_w16_d196_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer8_out_V_data_9_V_U(fifo_w16_d196_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer8_out_V_data_10_V_U(fifo_w16_d196_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer8_out_V_data_11_V_U(fifo_w16_d196_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer8_out_V_data_12_V_U(fifo_w16_d196_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer8_out_V_data_13_V_U(fifo_w16_d196_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer8_out_V_data_14_V_U(fifo_w16_d196_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer8_out_V_data_15_V_U(fifo_w16_d196_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer9_out_V_data_0_V_U(fifo_w16_d144_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer9_out_V_data_1_V_U(fifo_w16_d144_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer9_out_V_data_2_V_U(fifo_w16_d144_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer9_out_V_data_3_V_U(fifo_w16_d144_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer9_out_V_data_4_V_U(fifo_w16_d144_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer9_out_V_data_5_V_U(fifo_w16_d144_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer9_out_V_data_6_V_U(fifo_w16_d144_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer9_out_V_data_7_V_U(fifo_w16_d144_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer10_out_V_data_0_V_U(fifo_w16_d144_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer10_out_V_data_1_V_U(fifo_w16_d144_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer10_out_V_data_2_V_U(fifo_w16_d144_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer10_out_V_data_3_V_U(fifo_w16_d144_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer10_out_V_data_4_V_U(fifo_w16_d144_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer10_out_V_data_5_V_U(fifo_w16_d144_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer10_out_V_data_6_V_U(fifo_w16_d144_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer10_out_V_data_7_V_U(fifo_w16_d144_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer11_out_V_data_0_V_U(fifo_w6_d144_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer11_out_V_data_1_V_U(fifo_w6_d144_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer11_out_V_data_2_V_U(fifo_w6_d144_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer11_out_V_data_3_V_U(fifo_w6_d144_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer11_out_V_data_4_V_U(fifo_w6_d144_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer11_out_V_data_5_V_U(fifo_w6_d144_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer11_out_V_data_6_V_U(fifo_w6_d144_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer11_out_V_data_7_V_U(fifo_w6_d144_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer12_out_V_data_0_V_U(fifo_w16_d100_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer12_out_V_data_1_V_U(fifo_w16_d100_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer12_out_V_data_2_V_U(fifo_w16_d100_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer12_out_V_data_3_V_U(fifo_w16_d100_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer12_out_V_data_4_V_U(fifo_w16_d100_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer12_out_V_data_5_V_U(fifo_w16_d100_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer12_out_V_data_6_V_U(fifo_w16_d100_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer12_out_V_data_7_V_U(fifo_w16_d100_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer13_out_V_data_0_V_U(fifo_w16_d100_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer13_out_V_data_1_V_U(fifo_w16_d100_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer13_out_V_data_2_V_U(fifo_w16_d100_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer13_out_V_data_3_V_U(fifo_w16_d100_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer13_out_V_data_4_V_U(fifo_w16_d100_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer13_out_V_data_5_V_U(fifo_w16_d100_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer13_out_V_data_6_V_U(fifo_w16_d100_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer13_out_V_data_7_V_U(fifo_w16_d100_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer14_out_V_data_0_V_U(fifo_w6_d100_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer14_out_V_data_1_V_U(fifo_w6_d100_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer14_out_V_data_2_V_U(fifo_w6_d100_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer14_out_V_data_3_V_U(fifo_w6_d100_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer14_out_V_data_4_V_U(fifo_w6_d100_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer14_out_V_data_5_V_U(fifo_w6_d100_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer14_out_V_data_6_V_U(fifo_w6_d100_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer14_out_V_data_7_V_U(fifo_w6_d100_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer15_out_V_data_0_V_U(fifo_w16_d25_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer15_out_V_data_1_V_U(fifo_w16_d25_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer15_out_V_data_2_V_U(fifo_w16_d25_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer15_out_V_data_3_V_U(fifo_w16_d25_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer15_out_V_data_4_V_U(fifo_w16_d25_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer15_out_V_data_5_V_U(fifo_w16_d25_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer15_out_V_data_6_V_U(fifo_w16_d25_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'layer15_out_V_data_7_V_U(fifo_w16_d25_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_linear_array_array_ap_fixed_16u_linear_config3_U0_U(start_for_linear_array_array_ap_fixed_16u_linear_config3_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_relu_array_array_ap_fixed_16u_relu_config4_U0_U(start_for_relu_array_array_ap_fixed_16u_relu_config4_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_conv_2d_cl_array_array_ap_fixed_16u_config5_U0_U(start_for_conv_2d_cl_array_array_ap_fixed_16u_config5_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_linear_array_array_ap_fixed_16u_linear_config6_U0_U(start_for_linear_array_array_ap_fixed_16u_linear_config6_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_relu_array_array_ap_fixed_16u_relu_config7_U0_U(start_for_relu_array_array_ap_fixed_16u_relu_config7_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_pooling2d_cl_array_array_ap_fixed_16u_config8_U0_U(start_for_pooling2d_cl_array_array_ap_fixed_16u_config8_U0)' using Shift Registers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_conv_2d_cl_array_array_ap_fixed_8u_config9_U0_U(start_for_conv_2d_cl_array_array_ap_fixed_8u_config9_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_linear_array_array_ap_fixed_8u_linear_config10_U0_U(start_for_linear_array_array_ap_fixed_8u_linear_config10_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_relu_array_array_ap_fixed_8u_relu_config11_U0_U(start_for_relu_array_array_ap_fixed_8u_relu_config11_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_conv_2d_cl_array_array_ap_fixed_8u_config12_U0_U(start_for_conv_2d_cl_array_array_ap_fixed_8u_config12_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_linear_array_array_ap_fixed_8u_linear_config13_U0_U(start_for_linear_array_array_ap_fixed_8u_linear_config13_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_relu_array_array_ap_fixed_8u_relu_config14_U0_U(start_for_relu_array_array_ap_fixed_8u_relu_config14_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_pooling2d_cl_array_array_ap_fixed_8u_config15_U0_U(start_for_pooling2d_cl_array_array_ap_fixed_8u_config15_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_repack_stream_array_array_ap_fixed_200u_200_U0_U(start_for_repack_stream_array_array_ap_fixed_200u_200_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_dense_array_array_ap_fixed_16_6_5_3_0_10u_confibZs_U(start_for_dense_array_array_ap_fixed_16_6_5_3_0_10u_confibZs)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_linear_array_array_ap_fixed_10u_linear_config18b0s_U(start_for_linear_array_array_ap_fixed_10u_linear_config18b0s)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_softmax_array_array_ap_fixed_10u_softmax_configb1s_U(start_for_softmax_array_array_ap_fixed_10u_softmax_configb1s)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'in_local_V_data_0_V_U(fifo_w16_d3072_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'in_local_V_data_1_V_U(fifo_w16_d3072_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'in_local_V_data_2_V_U(fifo_w16_d3072_A)' using Block RAMs.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'is_last_0_i_loc_channel_U(fifo_w1_d2_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'out_local_V_data_0_V_U(fifo_w16_d10_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'out_local_V_data_1_V_U(fifo_w16_d10_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'out_local_V_data_2_V_U(fifo_w16_d10_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'out_local_V_data_3_V_U(fifo_w16_d10_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'out_local_V_data_4_V_U(fifo_w16_d10_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'out_local_V_data_5_V_U(fifo_w16_d10_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'out_local_V_data_6_V_U(fifo_w16_d10_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'out_local_V_data_7_V_U(fifo_w16_d10_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'out_local_V_data_8_V_U(fifo_w16_d10_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'out_local_V_data_9_V_U(fifo_w16_d10_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'tmp_data_V_0_U(fifo_w16_d2_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'tmp_data_V_1_U(fifo_w16_d2_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'tmp_data_V_2_U(fifo_w16_d2_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'tmp_data_V_3_U(fifo_w16_d2_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'tmp_data_V_4_U(fifo_w16_d2_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'tmp_data_V_5_U(fifo_w16_d2_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'tmp_data_V_6_U(fifo_w16_d2_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'tmp_data_V_7_U(fifo_w16_d2_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'tmp_data_V_8_U(fifo_w16_d2_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'tmp_data_V_9_U(fifo_w16_d2_A)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_myproject_cifar10_cnn4_U0_U(start_for_myproject_cifar10_cnn4_U0)' using Shift Registers.\n",
      "INFO: [RTMG 210-285] Implementing FIFO 'start_for_Block_myproject_cifar10_cnn4_axi_exit52_proc_U0_U(start_for_Block_myproject_cifar10_cnn4_axi_exit52_proc_U0)' using Shift Registers.\n",
      "INFO: [HLS 200-111] Finished generating all RTL models Time (s): cpu = 00:25:24 ; elapsed = 01:05:01 . Memory (MB): peak = 6586.586 ; gain = 6155.066 ; free physical = 825 ; free virtual = 166548\n",
      "INFO: [VHDL 208-304] Generating VHDL RTL for myproject_cifar10_cnn4_axi.\n",
      "INFO: [VLOG 209-307] Generating Verilog RTL for myproject_cifar10_cnn4_axi.\n",
      "***** C/RTL SYNTHESIS COMPLETED IN 1h4m12s *****\n",
      "***** EXPORT IP *****\n",
      "INFO: [IMPL 213-8] Exporting RTL as a Vivado IP.\n",
      "\n",
      "****** Vivado v2019.2 (64-bit)\n",
      "  **** SW Build 2708876 on Wed Nov  6 21:39:14 MST 2019\n",
      "  **** IP Build 2700528 on Thu Nov  7 00:09:20 MST 2019\n",
      "    ** Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.\n",
      "\n",
      "source run_ippack.tcl -notrace\n",
      "create_project: Time (s): cpu = 00:00:04 ; elapsed = 00:00:40 . Memory (MB): peak = 1512.273 ; gain = 63.016 ; free physical = 9753 ; free virtual = 175475\n",
      "INFO: [IP_Flow 19-234] Refreshing IP repositories\n",
      "INFO: [IP_Flow 19-1704] No user IP repositories specified\n",
      "INFO: [IP_Flow 19-2313] Loaded Vivado IP repository '/workspace/home/Xilinx/Vivado/2019.2/data/ip'.\n",
      "WARNING: [IP_Flow 19-4832] The IP name 'myproject_cifar10_cnn4_axi_ap_fpext_0_no_dsp_32' you have specified is long. The Windows operating system has path length limitations. It is recommended you use shorter names to reduce the likelihood of issues.\n",
      "create_ip: Time (s): cpu = 00:00:04 ; elapsed = 00:00:33 . Memory (MB): peak = 1605.449 ; gain = 93.176 ; free physical = 9477 ; free virtual = 175330\n",
      "INFO: [IP_Flow 19-1686] Generating 'Synthesis' target for IP 'myproject_cifar10_cnn4_axi_ap_fpext_0_no_dsp_32'...\n",
      "INFO: [IP_Flow 19-1686] Generating 'Simulation' target for IP 'myproject_cifar10_cnn4_axi_ap_fpext_0_no_dsp_32'...\n",
      "INFO: [IP_Flow 19-234] Refreshing IP repositories\n",
      "INFO: [IP_Flow 19-1704] No user IP repositories specified\n",
      "INFO: [IP_Flow 19-2313] Loaded Vivado IP repository '/workspace/home/Xilinx/Vivado/2019.2/data/ip'.\n",
      "ipx::update_checksums: Time (s): cpu = 00:00:00.72 ; elapsed = 00:00:34 . Memory (MB): peak = 1608.418 ; gain = 0.000 ; free physical = 9424 ; free virtual = 175444\n",
      "ipx::archive_core: Time (s): cpu = 00:00:00.56 ; elapsed = 00:00:07 . Memory (MB): peak = 1608.418 ; gain = 0.000 ; free physical = 9438 ; free virtual = 175440\n",
      "INFO: [Common 17-206] Exiting Vivado at Thu Dec  2 03:37:28 2021...\n",
      "***** EXPORT IP COMPLETED IN 0h8m54s *****\n",
      "INFO: [HLS 200-112] Total elapsed time: 4437.35 seconds; peak allocated memory: 1.985 GB.\n",
      "INFO: [Common 17-206] Exiting vivado_hls at Thu Dec  2 03:37:32 2021...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'EstimatedClockPeriod': '4.374',\n",
       " 'BestLatency': '14337',\n",
       " 'WorstLatency': '14337',\n",
       " 'IntervalMin': '14338',\n",
       " 'IntervalMax': '14338',\n",
       " 'BRAM_18K': '179',\n",
       " 'DSP48E': '122',\n",
       " 'FF': '47786',\n",
       " 'LUT': '216056',\n",
       " 'URAM': '0',\n",
       " 'AvailableBRAM_18K': '624',\n",
       " 'AvailableDSP48E': '1728',\n",
       " 'AvailableFF': '460800',\n",
       " 'AvailableLUT': '230400',\n",
       " 'AvailableURAM': '96'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PATH'] = '/workspace/home/Xilinx/Vivado/2019.2/bin:' + os.environ['PATH']\n",
    "hls_model.build(csim=False,synth=True,export=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87af97f6",
   "metadata": {},
   "source": [
    "## Check the reports\n",
    "Print out the reports generated by Vivado HLS. Pay attention to the Utilization Estimates' section in particular this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39f69a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 solution(s) in cifar10-hls-test-1/myproject_cifar10_cnn4_prj.\n",
      "Reports for solution \"solution1\":\n",
      "\n",
      "C simulation report not found.\n",
      "SYNTHESIS REPORT:\n",
      "================================================================\n",
      "== Vivado HLS Report for 'myproject_cifar10_cnn4_axi'\n",
      "================================================================\n",
      "* Date:           Tue Nov 30 04:14:07 2021\n",
      "\n",
      "* Version:        2019.2 (Build 2704478 on Wed Nov 06 22:10:23 MST 2019)\n",
      "* Project:        myproject_cifar10_cnn4_prj\n",
      "* Solution:       solution1\n",
      "* Product family: zynquplus\n",
      "* Target device:  xczu7ev-ffvc1156-2-e\n",
      "\n",
      "\n",
      "================================================================\n",
      "== Performance Estimates\n",
      "================================================================\n",
      "+ Timing: \n",
      "    * Summary: \n",
      "    +--------+---------+----------+------------+\n",
      "    |  Clock |  Target | Estimated| Uncertainty|\n",
      "    +--------+---------+----------+------------+\n",
      "    |ap_clk  | 5.00 ns | 4.371 ns |   0.62 ns  |\n",
      "    +--------+---------+----------+------------+\n",
      "\n",
      "+ Latency: \n",
      "    * Summary: \n",
      "    +---------+---------+-----------+-----------+-------+-------+----------+\n",
      "    |  Latency (cycles) |   Latency (absolute)  |    Interval   | Pipeline |\n",
      "    |   min   |   max   |    min    |    max    |  min  |  max  |   Type   |\n",
      "    +---------+---------+-----------+-----------+-------+-------+----------+\n",
      "    |    14337|    14337| 71.685 us | 71.685 us |  14338|  14338| dataflow |\n",
      "    +---------+---------+-----------+-----------+-------+-------+----------+\n",
      "\n",
      "    + Detail: \n",
      "        * Instance: \n",
      "        +-------------------------------------------------+----------------------------------------------+---------+---------+-----------+-----------+-------+-------+----------+\n",
      "        |                                                 |                                              |  Latency (cycles) |   Latency (absolute)  |    Interval   | Pipeline |\n",
      "        |                     Instance                    |                    Module                    |   min   |   max   |    min    |    max    |  min  |  max  |   Type   |\n",
      "        +-------------------------------------------------+----------------------------------------------+---------+---------+-----------+-----------+-------+-------+----------+\n",
      "        |myproject_cifar10_cnn4_U0                        |myproject_cifar10_cnn4                        |     2632|     2632| 13.160 us | 13.160 us |   2604|   2604| dataflow |\n",
      "        |Loop_1_proc541_U0                                |Loop_1_proc541                                |    14337|    14337| 71.685 us | 71.685 us |  14337|  14337|   none   |\n",
      "        |Loop_2_proc_U0                                   |Loop_2_proc                                   |       41|       41|  0.205 us |  0.205 us |     41|     41|   none   |\n",
      "        |Block_myproject_cifar10_cnn4_axi_exit52_proc_U0  |Block_myproject_cifar10_cnn4_axi_exit52_proc  |        0|        0|    0 ns   |    0 ns   |      0|      0|   none   |\n",
      "        +-------------------------------------------------+----------------------------------------------+---------+---------+-----------+-----------+-------+-------+----------+\n",
      "\n",
      "        * Loop: \n",
      "        N/A\n",
      "\n",
      "\n",
      "\n",
      "================================================================\n",
      "== Utilization Estimates\n",
      "================================================================\n",
      "* Summary: \n",
      "+-----------------+---------+-------+--------+--------+-----+\n",
      "|       Name      | BRAM_18K| DSP48E|   FF   |   LUT  | URAM|\n",
      "+-----------------+---------+-------+--------+--------+-----+\n",
      "|DSP              |        -|      -|       -|       -|    -|\n",
      "|Expression       |        -|      -|       0|      44|    -|\n",
      "|FIFO             |       12|      -|     349|    1142|    -|\n",
      "|Instance         |      167|    101|   39380|  106897|    0|\n",
      "|Memory           |        -|      -|       -|       -|    -|\n",
      "|Multiplexer      |        -|      -|       -|      90|    -|\n",
      "|Register         |        -|      -|      10|       -|    -|\n",
      "+-----------------+---------+-------+--------+--------+-----+\n",
      "|Total            |      179|    101|   39739|  108173|    0|\n",
      "+-----------------+---------+-------+--------+--------+-----+\n",
      "|Available        |      624|   1728|  460800|  230400|   96|\n",
      "+-----------------+---------+-------+--------+--------+-----+\n",
      "|Utilization (%)  |       28|      5|       8|      46|    0|\n",
      "+-----------------+---------+-------+--------+--------+-----+\n",
      "\n",
      "+ Detail: \n",
      "    * Instance: \n",
      "    +-------------------------------------------------+----------------------------------------------+---------+-------+-------+--------+-----+\n",
      "    |                     Instance                    |                    Module                    | BRAM_18K| DSP48E|   FF  |   LUT  | URAM|\n",
      "    +-------------------------------------------------+----------------------------------------------+---------+-------+-------+--------+-----+\n",
      "    |Block_myproject_cifar10_cnn4_axi_exit52_proc_U0  |Block_myproject_cifar10_cnn4_axi_exit52_proc  |        0|      0|    162|     193|    0|\n",
      "    |Loop_1_proc541_U0                                |Loop_1_proc541                                |        0|      0|    297|    1703|    0|\n",
      "    |Loop_2_proc_U0                                   |Loop_2_proc                                   |        0|      0|    147|    1031|    0|\n",
      "    |myproject_cifar10_cnn4_U0                        |myproject_cifar10_cnn4                        |      167|    101|  38774|  103970|    0|\n",
      "    +-------------------------------------------------+----------------------------------------------+---------+-------+-------+--------+-----+\n",
      "    |Total                                            |                                              |      167|    101|  39380|  106897|    0|\n",
      "    +-------------------------------------------------+----------------------------------------------+---------+-------+-------+--------+-----+\n",
      "\n",
      "Co-simulation report not found.\n"
     ]
    }
   ],
   "source": [
    "hls4ml.report.read_vivado_report(config['OutputDir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128da6a2",
   "metadata": {},
   "source": [
    "Print the report for the model trained in part 1. Now, compared to the model from part 1, this model has been trained with low-precision quantization, and 75% pruning. You should be able to see that we have saved a lot of resource compared to where we started in part 1. At the same time, referring to the ROC curve above, the model performance is pretty much identical even with this drastic compression!\n",
    "\n",
    "**Note you need to have trained and synthesized the model from part 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62b13853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 solution(s) in cifar10-hls-test/myproject_mnist_cnn4_prj.\n",
      "Reports for solution \"solution1\":\n",
      "\n",
      "C simulation report not found.\n",
      "Synthesis report not found.\n",
      "Co-simulation report not found.\n"
     ]
    }
   ],
   "source": [
    "hls4ml.report.read_vivado_report('cifar10-hls-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821e8bf2",
   "metadata": {},
   "source": [
    "Print the report for the model trained in part 3. Both these models were trained with 75% sparsity, but the new model uses 6-bit precision as well. You can see how Vivado HLS has moved multiplication operations from DSPs into LUTs, reducing the \"critical\" resource usage.\n",
    "\n",
    "**Note you need to have trained and synthesized the model from part 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f74e77ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 solution(s) in cifar10-hls-test/myproject_mnist_cnn4_prj.\n",
      "Reports for solution \"solution1\":\n",
      "\n",
      "C simulation report not found.\n",
      "Synthesis report not found.\n",
      "Co-simulation report not found.\n"
     ]
    }
   ],
   "source": [
    "hls4ml.report.read_vivado_report('cifar10-hls-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e08870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
